{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPREPARAÇÃO DOS DADOS\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "PREPARAÇÃO DOS DADOS\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparações realizadas com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize  \n",
    "import os, re, operator\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "from src.utils import *\n",
    "from src.k_nearest_neighbors import *\n",
    "from src.logistic_regression import *\n",
    "from src.naive_bayes import *\n",
    "from src.neural_network import *\n",
    "#from src.support_vector_machines import *\n",
    "from src.validation import *\n",
    "\n",
    "#Base de dados\n",
    "POSITIVE_TRUTHFUL  = ['op_spam_v1.4/positive/truthful/']\n",
    "POSITIVE_DECEPTIVE = ['op_spam_v1.4/positive/deceptive/']\n",
    "NEGATIVE_TRUTHFUL  = ['op_spam_v1.4/negative/truthful/']\n",
    "NEGATIVE_DECEPTIVE = ['op_spam_v1.4/negative/deceptive/']\n",
    "POSITIVE = [POSITIVE_TRUTHFUL[0], POSITIVE_DECEPTIVE[0]]\n",
    "NEGATIVE = [NEGATIVE_TRUTHFUL[0], NEGATIVE_DECEPTIVE[0]]\n",
    "DECEPTIVE = [POSITIVE_DECEPTIVE[0], NEGATIVE_DECEPTIVE[0]]\n",
    "TRUTHFUL = [POSITIVE_TRUTHFUL[0], NEGATIVE_TRUTHFUL[0]]\n",
    "ALL = [POSITIVE_TRUTHFUL[0], POSITIVE_DECEPTIVE[0],\n",
    "           NEGATIVE_TRUTHFUL[0], NEGATIVE_DECEPTIVE[0]]\n",
    "\n",
    "#Parametros\n",
    "    #Pre-processamento\n",
    "USE_NGRAM = True\n",
    "REMOVE_STOPWORD = True\n",
    "REMOVE_ABOVE_THRESHOLD = 3\n",
    "MIN_GRAM = 1\n",
    "MAX_GRAM = 2\n",
    "NORMALIZE = False\n",
    "SAVE = False\n",
    "\n",
    "    #SVM\n",
    "COST = [i*0.3+0.1 for i in range(0, 100)]\n",
    "GAMMA = [i*0.3+0.1 for i in range(0, 100)]\n",
    "KERNEL = 2\n",
    "\n",
    "    #FOLD\n",
    "K_FOLD = 5\n",
    "\n",
    "    #Outras constants\n",
    "EMPTY = ''\n",
    "ZERO = 0\n",
    "ONE = 1\n",
    "ENGLISH = 'english'\n",
    "STOPWORDS = 'stopwords'\n",
    "\n",
    "if K_FOLD == ZERO:\n",
    "    raise Exception('Error on parameter for generate folds.')\n",
    "if MIN_GRAM > MAX_GRAM or MIN_GRAM < ONE or MAX_GRAM < ONE:\n",
    "    raise Exception('Error on parameter for N-Gram.')\n",
    "    \n",
    "print('Preparações realizadas com sucesso.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Bruno\n",
      "[nltk_data]     Morii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Remoção de Stop Words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download do dicionário de stop wordsgreat\n",
    "nltk.download(STOPWORDS)\n",
    "\n",
    "def stopwords_removal(tokens_list):\n",
    "    '''\n",
    "        Dado uma lista de tokens.\n",
    "        Então é removido as stopwords.\n",
    "        \n",
    "        Entrada: tokens_list - Uma lista de tokens.\n",
    "        Saida: new_tokens_list - Uma lista de tokens.\n",
    "    '''\n",
    "    if len(tokens_list) == ZERO:\n",
    "        raise Exception('Error on stopwords_removal.')\n",
    "        \n",
    "    new_tokens_list = []\n",
    "    \n",
    "    stop_words = stopwords.words(ENGLISH)\n",
    "    new_tokens_list = [token for token in tokens_list if token not in stop_words]\n",
    "    \n",
    "    if len(new_tokens_list) == ZERO:\n",
    "        raise Exception('Error on stopwords_removal.')\n",
    "        \n",
    "    return new_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram dos tokens\n",
    "def generate_ngram(tokens_list):\n",
    "    '''\n",
    "        Dado uma lista de tokens.\n",
    "        Então é gerado N-Gram com base no MAX_GRAM e MIN_GRAM.\n",
    "        \n",
    "        Entrada: tokens_list - Uma lista de tokens.\n",
    "        Saida: allNGrams - Uma lista de N-Gram.\n",
    "    '''\n",
    "    if len(tokens_list) == ZERO:\n",
    "        raise Exception('Error on generate_ngram.')\n",
    "        \n",
    "    allNGrams = []\n",
    "    \n",
    "    #N-Gram\n",
    "    for idx in range(MIN_GRAM, MAX_GRAM+1):\n",
    "        ngrams = zip(*[tokens_list[i:] for i in range(idx)])\n",
    "        allNGrams += ([\" \".join(ngram) for ngram in ngrams])\n",
    "    \n",
    "    if len(allNGrams) == ZERO:\n",
    "        raise Exception('Error on generate_ngram.')\n",
    "        \n",
    "    return allNGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remoção de features irrelevantes por threshold\n",
    "def features_removal(features_dic, threshold):\n",
    "    '''\n",
    "        Dado um dicionario de features e um threshold.\n",
    "        Então o dicionario é filtrado pelo numero do threshold.\n",
    "        \n",
    "        Entrada: features_dic - Um dicionario de features.\n",
    "                 threshold - Um inteiro para limitar o minimo de ocorrencias de um item do dicionario.\n",
    "        Saida: new_features_list - Um dicionario de features filtradas.\n",
    "               new_features_dic - Um dicionario com as features filtradas.\n",
    "    '''\n",
    "    if len(features_dic) == ZERO or threshold < ONE:\n",
    "        raise Exception('Error on features_removal.')\n",
    "\n",
    "    new_features_dic = {}\n",
    "    \n",
    "    #Remove values < threshold\n",
    "    for key, value in features_dic.items():\n",
    "        if value > threshold:\n",
    "            new_features_dic[key] = value\n",
    "\n",
    "    new_features_list = [k for k, v in new_features_dic.items()]\n",
    "    new_features_dic = sorted(new_features_dic.items(), key=operator.itemgetter(1))\n",
    "    \n",
    "    if len(new_features_list) == ZERO or len(new_features_dic) == ZERO:\n",
    "        raise Exception('Error on features_removal.')\n",
    "        \n",
    "    return new_features_dic, new_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pré processamento em andamento...\n",
      "Pré-processamento finalizado.\n",
      "Duração: 0:00:00.534462\n",
      "Foram gerados 3294 features.\n"
     ]
    }
   ],
   "source": [
    "# Tokenização dos textos\n",
    "def pre_processing(paths, stopwords=False, ngram=False, threshold=0):\n",
    "    '''\n",
    "        Dado arquivos de textos.\n",
    "        Então o texto é limpo (são removidos pontuação) e\n",
    "            aplicado alguns métodos como remoção de stopwords, N-Gram e filtragem por threshold.\n",
    "        \n",
    "        Entrada: paths - Um dicionario de features.\n",
    "                 stopwords - Um boolean indicando se é para utilizar este método.\n",
    "                 ngram - Um boolean indicando se é para utilizar este método.\n",
    "                 threshold - Um inteiro maior que zero que é utilizado na função features_removal.\n",
    "        Saida: token_list - Uma lista contendo todas as features.\n",
    "               token_dic - Um dicionario com as features e a sua ocorrencia.\n",
    "    '''\n",
    "    if len(paths) == ZERO:\n",
    "        raise Exception('Error on pre_processing.')\n",
    "        \n",
    "    token_dic = {}\n",
    "    dirs =[]\n",
    "    token_list = []\n",
    "    \n",
    "    #Juntando arquivos de texto\n",
    "    for p in paths:\n",
    "        text_path = [p + t for t in os.listdir(p)]\n",
    "        dirs += (text_path)\n",
    "    \n",
    "    for texts in dirs:\n",
    "        with open(texts, 'r', encoding='utf-8') as stream:\n",
    "            tokens = []\n",
    "            text = stream.read()\n",
    "\n",
    "            #Tokenizando o texto\n",
    "            tokens = [word for word in (re.sub(r'[^\\w\\s]+','', text.replace('\\n','')).lower().split(' ')) if word != '']\n",
    "            \n",
    "            #Tratando stopwords\n",
    "            if stopwords:\n",
    "                tokens = stopwords_removal(tokens)\n",
    "            \n",
    "            #Aplicando ngram\n",
    "            if ngram:\n",
    "                tokens = generate_ngram(tokens)\n",
    "            \n",
    "            #Criando dicionario de palavras\n",
    "            for token in tokens:\n",
    "                if token in token_dic:\n",
    "                    token_dic[token] += 1\n",
    "                else:\n",
    "                    token_dic[token] = 1\n",
    "    \n",
    "    #Removendo palavras com base no threshold\n",
    "    if threshold > ZERO:\n",
    "        token_dic, token_list = features_removal(token_dic, threshold)\n",
    "    \n",
    "    if len(token_dic) == ZERO or len(token_list) == ZERO:\n",
    "        raise Exception('Error on pre_processing.')\n",
    "        \n",
    "    return token_dic, token_list\n",
    "print('Pré processamento em andamento...')\n",
    "start_time = datetime.now()\n",
    "dicFeatures, listFeatures = pre_processing(DECEPTIVE, REMOVE_STOPWORD, USE_NGRAM, REMOVE_ABOVE_THRESHOLD)\n",
    "print('Pré-processamento finalizado.')\n",
    "end_time = datetime.now()\n",
    "print('Duração: {}'.format(end_time - start_time))\n",
    "print('Foram gerados {} features.'.format(len(listFeatures)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tratamento em andamento...\n",
      "Duração - ND: 0:00:10.709019\n",
      "Duração - NT: 0:00:22.032572\n",
      "Duração - PD: 0:00:29.350853\n",
      "Duração - PT: 0:00:37.584975\n",
      "Tratamento finalizado.\n"
     ]
    }
   ],
   "source": [
    "#Gera a matriz de features X e o vetor de classes Y\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "def generateFeatures(paths, listFeatures, stopwords=False, ngram=False, threshold=0, example_class=0):\n",
    "    '''\n",
    "        Dado arquivos de textos.\n",
    "        Então o dicionario é filtrado pelo numero do threshold\n",
    "            e é gerado uma matriz que faz a contagem das features por texto.\n",
    "        \n",
    "        Entrada: paths - Textos a serem analizados para a geração das features.\n",
    "                 listFeatures - Uma lista de features.\n",
    "                 stopwords - Um boolean indicando se é para utilizar este método.\n",
    "                 ngram - Um boolean indicando se é para utilizar este método.\n",
    "                 threshold - Um inteiro maior que zero que é utilizado na função features_removal.\n",
    "        Saida: Nenhuma.\n",
    "    '''\n",
    "    if len(paths) == ZERO:\n",
    "        raise Exception('Error on generateFeatures.')\n",
    "        \n",
    "    dirs = []\n",
    "    \n",
    "    for p in paths:\n",
    "        text_path = [p + t for t in os.listdir(p)]\n",
    "        dirs += (text_path)\n",
    "    \n",
    "    for texts in dirs:\n",
    "        row = np.zeros(len(listFeatures))\n",
    "        with open(texts, 'r', encoding='utf-8') as stream:\n",
    "            tokens = []\n",
    "            text = stream.read()\n",
    "\n",
    "            #Pre processing\n",
    "            tokens = [word for word in (re.sub(r'[^\\w\\s]+','', text.replace('\\n','')).lower().split(' ')) if word != '']\n",
    "            \n",
    "            if stopwords:\n",
    "                tokens = stopwords_removal(tokens)\n",
    "            \n",
    "            if ngram:\n",
    "                tokens = generate_ngram(tokens)\n",
    "            \n",
    "            for token in tokens:\n",
    "                for idx, feature in enumerate(listFeatures):\n",
    "                    if token == feature:\n",
    "                        row[idx] += 1\n",
    "                        \n",
    "            Y.append(example_class)\n",
    "            X.append(row)\n",
    "            \n",
    "start_time = datetime.now()\n",
    "print('Tratamento em andamento...')\n",
    "\n",
    "generateFeatures(NEGATIVE_DECEPTIVE, listFeatures, REMOVE_STOPWORD, USE_NGRAM, REMOVE_ABOVE_THRESHOLD, 1)\n",
    "end_time = datetime.now()\n",
    "print('Duração - ND: {}'.format(end_time - start_time))\n",
    "\n",
    "generateFeatures(NEGATIVE_TRUTHFUL, listFeatures, REMOVE_STOPWORD, USE_NGRAM, REMOVE_ABOVE_THRESHOLD, 0)\n",
    "end_time = datetime.now()\n",
    "print('Duração - NT: {}'.format(end_time - start_time))\n",
    "\n",
    "generateFeatures(POSITIVE_DECEPTIVE, listFeatures, REMOVE_STOPWORD, USE_NGRAM, REMOVE_ABOVE_THRESHOLD, 1)\n",
    "end_time = datetime.now()\n",
    "print('Duração - PD: {}'.format(end_time - start_time))\n",
    "\n",
    "generateFeatures(POSITIVE_TRUTHFUL, listFeatures, REMOVE_STOPWORD, USE_NGRAM, REMOVE_ABOVE_THRESHOLD, 0)\n",
    "end_time = datetime.now()\n",
    "print('Duração - PT: {}'.format(end_time - start_time))\n",
    "\n",
    "#Transformando X e Y que são listas em array\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "print('Tratamento finalizado.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salva os dados\n",
    "if SAVE == True:\n",
    "    save(np.column_stack((X, Y)), listFeatures, 'matrix_x')\n",
    "    print('Dados salvos.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normaliza os dados\n",
    "X_norm = []\n",
    "mu = 0\n",
    "sigma = 0\n",
    "\n",
    "if NORMALIZE == True:\n",
    "    X_norm, mu, sigma = normalize(X)\n",
    "    print('Dados normalizados.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando folders...\n",
      "Foram gerados 5 fold(s)\n",
      "Duração: 0:00:00.001995\n",
      "\n",
      "Fold 0 - Treino: 1280\n",
      "Fold 0 - Teste:  320\n",
      "\n",
      "Fold 1 - Treino: 1280\n",
      "Fold 1 - Teste:  320\n",
      "\n",
      "Fold 2 - Treino: 1280\n",
      "Fold 2 - Teste:  320\n",
      "\n",
      "Fold 3 - Treino: 1280\n",
      "Fold 3 - Teste:  320\n",
      "\n",
      "Fold 4 - Treino: 1280\n",
      "Fold 4 - Teste:  320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Gera os folds necessários para os treinamentos \n",
    "start_time = datetime.now()\n",
    "print('Gerando folders...')\n",
    "folds = stratified_kfolds(Y, K_FOLD, np.unique(Y))\n",
    "end_time = datetime.now()\n",
    "\n",
    "\n",
    "print(\"Foram gerados {} fold(s)\".format(K_FOLD), end='\\n')\n",
    "print('Duração: {}\\n'.format(end_time - start_time))\n",
    "idx = 0\n",
    "for train_index, test_index in folds:\n",
    "    print('Fold {} - Treino: {}'.format(idx, len(train_index)))\n",
    "    print('Fold {} - Teste:  {}'.format(idx, len(test_index)), end='\\n\\n')\n",
    "    idx += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNAIVE BAYES\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "NAIVE BAYES\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes\n",
    "def multinomial_naiveBayes(folds, X, Y):\n",
    "    '''\n",
    "        Dado os folds, X e Y.\n",
    "        Então é executado o algoritmo naive bayes para cada fold.\n",
    "        \n",
    "        Entrada: folds - Folds para serem testados.\n",
    "                 X - Matriz de conjunto de dados.\n",
    "                 Y - Lista com as classes correspondente aos dados da matriz X.\n",
    "        Saida: resultados - Lista de resultados para cada fold.\n",
    "               classes - Lista com as classes baseada na lista Y.\n",
    "    '''\n",
    "    resultados = []\n",
    "    classes = np.unique(Y)\n",
    "\n",
    "    for train_index, test_index in folds:\n",
    "        # Treinamento\n",
    "        probsPos, probsNeg = mult_nb_calcular_probabilidades(X[train_index], Y[train_index])\n",
    "\n",
    "        # Classificação\n",
    "        pred = []\n",
    "        for x in X[test_index]:\n",
    "            pred.append(mult_nb_classificacao(x, probsPos, probsNeg, sum(Y == 1)/len(Y), sum(Y == 0)/len(Y)))\n",
    "\n",
    "        cm = get_confusionMatrix(Y[test_index], pred, np.unique(Y))\n",
    "        resultado = relatorioDesempenho(cm, classes, False)\n",
    "\n",
    "        resultados.append(resultado)\n",
    "    \n",
    "    return resultados, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Rotina de testes - NB\n",
    "#----------------------------\n",
    "start_time = datetime.now()\n",
    "print('Naive Bayes em andamento...')\n",
    "resultados, classes = multinomial_naiveBayes(folds, X, Y)\n",
    "mediaFolds(resultados, classes)\n",
    "end_time = datetime.now()\n",
    "print('Duração - Naive Bayes: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "def K_Nearest_Neighbors(folds, X, Y):\n",
    "    '''\n",
    "        Dado os folds, X e Y.\n",
    "        Então é executado o algoritmo KNN para cada fold.\n",
    "        \n",
    "        Entrada: folds - Folds para serem testados.\n",
    "                 X - Matriz de conjunto de dados.\n",
    "                 Y - Lista com as classes correspondente aos dados da matriz X.\n",
    "        Saida: resultados - Lista de resultados para cada fold.\n",
    "               classes - Lista com as classes baseada na lista Y.\n",
    "    '''\n",
    "    resultados = []\n",
    "    classes = np.unique(Y)\n",
    "\n",
    "    for train_index, test_index in folds:\n",
    "        # Classificação\n",
    "        pred = []\n",
    "        K = 3\n",
    "        for x in X[test_index]:\n",
    "            y, ind_viz = knn(x, X, Y, K)\n",
    "            pred.append(y)\n",
    "\n",
    "        cm = get_confusionMatrix(Y[test_index], pred, np.unique(Y))\n",
    "        resultado = relatorioDesempenho(cm, classes, False)\n",
    "\n",
    "        resultados.append(resultado)\n",
    "    \n",
    "    return resultados, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Rotina de testes - KNN\n",
    "#----------------------------\n",
    "print('KNN em andamento...')\n",
    "start_time = datetime.now()\n",
    "resultados, classes = K_Nearest_Neighbors(folds, X, Y)\n",
    "end_time = datetime.now()\n",
    "print('Duração - KNN: {}'.format(end_time - start_time))\n",
    "mediaFolds(resultados, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nREDE NEURAL\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "REDE NEURAL\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rede Neural\n",
    "def redeNeural(folds, X, Y, vLambda, tamanho_intermediaria):\n",
    "    '''\n",
    "        Dado os folds, X e Y, vLambda e tamanho_intermediaria.\n",
    "        Em que:\n",
    "            vLambda é coeficiente de regularização\n",
    "            tamanho_intermediaria é o número de neurônios na camada intemediária\n",
    "        Então é executado o algoritmo naive bayes para cada fold.\n",
    "        \n",
    "        Entrada: folds - Folds para serem testados.\n",
    "                 X - Matriz de conjunto de dados.\n",
    "                 Y - Lista com as classes correspondente aos dados da matriz X.\n",
    "                 \n",
    "        Saida: resultados - Lista de resultados para cada fold.\n",
    "               classes - Lista com as classes baseada na lista Y.\n",
    "    '''\n",
    "    resultados = []\n",
    "    \n",
    "    classes = np.unique(Y)\n",
    "    tamanho_entrada = X.shape[1]\n",
    "\n",
    "    \n",
    "    Theta1 = inicializaPesosAleatorios(tamanho_entrada, tamanho_intermediaria, randomSeed = 10);\n",
    "    Theta2 = inicializaPesosAleatorios(tamanho_intermediaria, 2, randomSeed = 20);\n",
    "    Thetas = np.concatenate([np.ravel(Theta1), np.ravel(Theta2)])\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    for train_index, test_index in folds:\n",
    "        \n",
    "        # Otimização dos Thetas\n",
    "        \n",
    "        Theta1, Theta2 = rna_treino(Thetas, tamanho_entrada, tamanho_intermediaria, 2, X[train_index], Y[train_index], vLambda)\n",
    "        save_raw(Theta1, 'RNAResults/' + 'v' + str(vLambda) + 'n' + str(tamanho_intermediaria) + 'Theta1')\n",
    "        save_raw(Theta2, 'RNAResults/' + 'v' + str(vLambda) + 'n' + str(tamanho_intermediaria) + 'Theta2')\n",
    "        \n",
    "        # Classificação\n",
    "        pred = rna_predicao(Theta1, Theta2, X[test_index])\n",
    "        #display(pred)\n",
    "\n",
    "        cm = get_confusionMatrix(Y[test_index], pred, np.unique(Y))\n",
    "        resultado = relatorioDesempenho(cm, classes, False)\n",
    "\n",
    "        resultados.append(resultado)\n",
    "        end_time = datetime.now()\n",
    "        print('Duração - Fold em RNA: {}'.format(end_time - start_time))\n",
    "    \n",
    "    return resultados, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1, 2, 3, 5, 7, 10, 15, 25],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [3, 0, 0, 0, 0, 0, 0, 0],\n",
       " [5, 0, 0, 0, 0, 0, 0, 0],\n",
       " [7, 0, 0, 0, 0, 0, 0, 0],\n",
       " [9, 0, 0, 0, 0, 0, 0, 0],\n",
       " [11, 0, 0, 0, 0, 0, 0, 0],\n",
       " [13, 0, 0, 0, 0, 0, 0, 0],\n",
       " [15, 0, 0, 0, 0, 0, 0, 0],\n",
       " [17, 0, 0, 0, 0, 0, 0, 0],\n",
       " [19, 0, 0, 0, 0, 0, 0, 0],\n",
       " [21, 0, 0, 0, 0, 0, 0, 0],\n",
       " [23, 0, 0, 0, 0, 0, 0, 0],\n",
       " [25, 0, 0, 0, 0, 0, 0, 0],\n",
       " [27, 0, 0, 0, 0, 0, 0, 0],\n",
       " [29, 0, 0, 0, 0, 0, 0, 0],\n",
       " [31, 0, 0, 0, 0, 0, 0, 0],\n",
       " [33, 0, 0, 0, 0, 0, 0, 0],\n",
       " [42, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'#############################'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Iniciando rotina de Testes...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'#############################'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'------------------------------------------------------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'RNA em andamento com vLambda = 1 e 2 neurônios na camada intermediária...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração - Fold em RNA: 0:00:05.077695\n",
      "Duração - Fold em RNA: 0:00:11.589752\n",
      "Duração - Fold em RNA: 0:00:17.733428\n",
      "Duração - Fold em RNA: 0:00:24.702350\n",
      "Duração - Fold em RNA: 0:00:31.000381\n",
      "Duraçãos Total - RNA: 0:00:31.001379\n",
      "\n",
      "\tRevocacao   Precisao   F-medida   Classe\n",
      "\t0.851       0.849      0.849      0\n",
      "\t0.847       0.852      0.849      1\n",
      "\t---------------------------------------------------------------------\n",
      "\t0.849       0.851      0.849      Média macro\n",
      "\t0.849       0.849      0.849      Média micro\n",
      "\n",
      "\tAcuracia: 0.849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[-1, 2, 3, 5, 7, 10, 15, 25],\n",
       " [1, 0.8493749999999999, 0, 0, 0, 0, 0, 0],\n",
       " [3, 0, 0, 0, 0, 0, 0, 0],\n",
       " [5, 0, 0, 0, 0, 0, 0, 0],\n",
       " [7, 0, 0, 0, 0, 0, 0, 0],\n",
       " [9, 0, 0, 0, 0, 0, 0, 0],\n",
       " [11, 0, 0, 0, 0, 0, 0, 0],\n",
       " [13, 0, 0, 0, 0, 0, 0, 0],\n",
       " [15, 0, 0, 0, 0, 0, 0, 0],\n",
       " [17, 0, 0, 0, 0, 0, 0, 0],\n",
       " [19, 0, 0, 0, 0, 0, 0, 0],\n",
       " [21, 0, 0, 0, 0, 0, 0, 0],\n",
       " [23, 0, 0, 0, 0, 0, 0, 0],\n",
       " [25, 0, 0, 0, 0, 0, 0, 0],\n",
       " [27, 0, 0, 0, 0, 0, 0, 0],\n",
       " [29, 0, 0, 0, 0, 0, 0, 0],\n",
       " [31, 0, 0, 0, 0, 0, 0, 0],\n",
       " [33, 0, 0, 0, 0, 0, 0, 0],\n",
       " [42, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'------------------------------------------------------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'RNA em andamento com vLambda = 1 e 3 neurônios na camada intermediária...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração - Fold em RNA: 0:00:07.466232\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-fe6b9e80204a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RNA em andamento com vLambda = '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvlambdas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' e '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minter_neuros\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' neurônios na camada intermediária...'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mresultados\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mredeNeural\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvlambdas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minter_neuros\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Duraçãos Total - RNA: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-2df560252e08>\u001b[0m in \u001b[0;36mredeNeural\u001b[1;34m(folds, X, Y, vLambda, tamanho_intermediaria)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# Otimização dos Thetas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mTheta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTheta2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrna_treino\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mThetas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtamanho_entrada\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtamanho_intermediaria\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvLambda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0msave_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTheta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RNAResults/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'v'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvLambda\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'n'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtamanho_intermediaria\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'Theta1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0msave_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTheta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RNAResults/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'v'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvLambda\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'n'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtamanho_intermediaria\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'Theta2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AM\\Trabalho\\machine_learn_project\\src\\neural_network.py\u001b[0m in \u001b[0;36mrna_treino\u001b[1;34m(Thetas, tamanho_entrada, tamanho_intermediaria, num_classes, X, Y, vLambda)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;31m# Minimiza a funcao de custo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     result = scipy.optimize.minimize(fun=funcaoCusto_backp_reg, x0=Thetas, args=(tamanho_entrada, tamanho_intermediaria, num_classes, X, Y, vLambda),  \n\u001b[1;32m--> 121\u001b[1;33m                     method='TNC', jac=True, options={'maxiter': MaxIter})\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;31m# Coleta os pesos retornados pela função de minimização\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programas\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n\u001b[1;32m--> 604\u001b[1;33m                              **options)\n\u001b[0m\u001b[0;32m    605\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cobyla'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_cobyla\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programas\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\tnc.py\u001b[0m in \u001b[0;36m_minimize_tnc\u001b[1;34m(fun, x0, args, jac, bounds, eps, scale, offset, mesg_num, maxCGit, maxiter, eta, stepmx, accuracy, minfev, ftol, xtol, gtol, rescale, disp, callback, **unknown_options)\u001b[0m\n\u001b[0;32m    407\u001b[0m                                         \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxCGit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxfun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m                                         \u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstepmx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mftol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m                                         xtol, pgtol, rescale, callback)\n\u001b[0m\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[0mfunv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjacv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programas\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\tnc.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    369\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programas\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\AM\\Trabalho\\machine_learn_project\\src\\neural_network.py\u001b[0m in \u001b[0;36mfuncaoCusto_backp_reg\u001b[1;34m(thetas, tamanho_entrada, tamanho_intermediaria, num_classes, X, y, vLambda)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;31m# === Foward Propagation ===\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m# insere bias = 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0mX_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;31m# rede oculta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programas\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36minsert\u001b[1;34m(arr, obj, values, axis)\u001b[0m\n\u001b[0;32m   4598\u001b[0m         \u001b[0mslobj2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4599\u001b[0m         \u001b[0mslobj2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4600\u001b[1;33m         \u001b[0mnew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslobj2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4601\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4602\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Rotina de testes - RNA\n",
    "#----------------------------\n",
    "vlambdas = [1, 3, 5 , 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 42]\n",
    "inter_neuros = [2, 3, 5, 7 ,10, 15, 25]\n",
    "#inter_neuros = [10]\n",
    "\n",
    "#cria matriz para salvar no csv\n",
    "save_matrix = [[0 for x in range(len(inter_neuros)+1)] for y in range(len(vlambdas)+1)]\n",
    "save_matrix[0][0] = -1\n",
    "for i in range(len(inter_neuros)):\n",
    "    save_matrix[0][i+1] = inter_neuros[i]\n",
    "for i in range(len(vlambdas)):\n",
    "    save_matrix[i+1][0] = vlambdas[i]\n",
    "display(save_matrix)\n",
    "\n",
    "display('#############################')\n",
    "display('Iniciando rotina de Testes...')\n",
    "display('#############################')\n",
    "start_all = datetime.now()\n",
    "for v in range(len(vlambdas)):\n",
    "    for i in range(len(inter_neuros)):\n",
    "        display('------------------------------------------------------------')\n",
    "        display('RNA em andamento com vLambda = ' + str(vlambdas[v]) + ' e ' + str(inter_neuros[i]) + ' neurônios na camada intermediária...' )\n",
    "        start_time = datetime.now()\n",
    "        resultados, classes = redeNeural(folds, X, Y, vlambdas[v], inter_neuros[i])\n",
    "        end_time = datetime.now()\n",
    "        print('Duraçãos Total - RNA: {}'.format(end_time - start_time))\n",
    "        save_matrix[v+1][i+1] = mediaFolds(resultados, classes)\n",
    "        display(save_matrix)\n",
    "        \n",
    "display('############################')\n",
    "display('Rotina de testes finalizadas')\n",
    "end_time = datetime.now()\n",
    "print('Duração Total - RNA: {}'.format(end_time - start_all))\n",
    "save_raw(save_matrix, 'RNAResults/Results')\n",
    "display('############################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_matrix = [[-1, 2, 3, 5, 7, 10, 15, 25],\n",
    " [1,\n",
    "  0.8493749999999999,\n",
    "  0.849375,\n",
    "  0.8475000000000001,\n",
    "  0.8481250000000001,\n",
    "  0.8481250000000001,\n",
    "  0.8475000000000001,\n",
    "  0.8487500000000001],\n",
    " [3, 0.850625, 0.85, 0.850625, 0.849375, 0.85, 0.8506250000000002, 0.85],\n",
    " [5,\n",
    "  0.849375,\n",
    "  0.85375,\n",
    "  0.8525,\n",
    "  0.8518749999999999,\n",
    "  0.85125,\n",
    "  0.853125,\n",
    "  0.853125],\n",
    " [7,\n",
    "  0.85375,\n",
    "  0.8537500000000001,\n",
    "  0.8549999999999999,\n",
    "  0.853125,\n",
    "  0.85375,\n",
    "  0.8543750000000001,\n",
    "  0.85625],\n",
    " [9,\n",
    "  0.85375,\n",
    "  0.85625,\n",
    "  0.8550000000000001,\n",
    "  0.85625,\n",
    "  0.8556249999999999,\n",
    "  0.8574999999999999,\n",
    "  0.85625],\n",
    " [11,\n",
    "  0.85625,\n",
    "  0.8568749999999999,\n",
    "  0.8556250000000001,\n",
    "  0.8556250000000001,\n",
    "  0.8556250000000001,\n",
    "  0.8556250000000001,\n",
    "  0.85625],\n",
    " [13,\n",
    "  0.85625,\n",
    "  0.8543749999999999,\n",
    "  0.8556250000000001,\n",
    "  0.85625,\n",
    "  0.8581249999999999,\n",
    "  0.8568749999999999,\n",
    "  0.85625],\n",
    " [15,\n",
    "  0.8574999999999999,\n",
    "  0.85625,\n",
    "  0.858125,\n",
    "  0.858125,\n",
    "  0.858125,\n",
    "  0.858125,\n",
    "  0.858125],\n",
    " [17, 0.8574999999999999, 0.85875, 0.859375, 0.859375, 0.85875, 0.86, 0.86],\n",
    " [19,\n",
    "  0.858125,\n",
    "  0.859375,\n",
    "  0.86,\n",
    "  0.86,\n",
    "  0.85625,\n",
    "  0.8581249999999999,\n",
    "  0.8581249999999999],\n",
    " [21,\n",
    "  0.85875,\n",
    "  0.8606250000000001,\n",
    "  0.8587499999999999,\n",
    "  0.8574999999999999,\n",
    "  0.8574999999999999,\n",
    "  0.8574999999999999,\n",
    "  0.8574999999999999],\n",
    " [23,\n",
    "  0.86,\n",
    "  0.8574999999999999,\n",
    "  0.8574999999999999,\n",
    "  0.8581249999999999,\n",
    "  0.85875,\n",
    "  0.85875,\n",
    "  0.8581249999999999],\n",
    " [25,\n",
    "  0.8574999999999999,\n",
    "  0.8568749999999999,\n",
    "  0.8574999999999999,\n",
    "  0.8568749999999999,\n",
    "  0.85625,\n",
    "  0.8568749999999999,\n",
    "  0.8574999999999999],\n",
    " [27,\n",
    "  0.8574999999999999,\n",
    "  0.85625,\n",
    "  0.8575000000000002,\n",
    "  0.858125,\n",
    "  0.859375,\n",
    "  0.859375,\n",
    "  0.85875],\n",
    " [29,\n",
    "  0.8581249999999999,\n",
    "  0.8581249999999999,\n",
    "  0.85875,\n",
    "  0.8574999999999999,\n",
    "  0.8574999999999999,\n",
    "  0.8575000000000002,\n",
    "  0.8575000000000002],\n",
    " [31,\n",
    "  0.8587499999999999,\n",
    "  0.8581249999999999,\n",
    "  0.8575000000000002,\n",
    "  0.8556250000000001,\n",
    "  0.85625,\n",
    "  0.8550000000000001,\n",
    "  0.8556250000000001],\n",
    " [33,\n",
    "  0.859375,\n",
    "  0.85625,\n",
    "  0.85625,\n",
    "  0.8574999999999999,\n",
    "  0.8543749999999999,\n",
    "  0.8543750000000001,\n",
    "  0.8543749999999999],\n",
    " [42,\n",
    "  0.8525,\n",
    "  0.8524999999999998,\n",
    "  0.850625,\n",
    "  0.849375,\n",
    "  0.85,\n",
    "  0.8487500000000001,\n",
    "  0.8481250000000001]]\n",
    "mat = np.array(save_matrix)\n",
    "mat = mat[1:,1:]\n",
    "display(np.argmax(np.mean(mat, axis=0)))\n",
    "display(np.argmax(np.mean(mat, axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "REGRESSAO LOGISTICA\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGRESSAO LOGISTICA\n",
    "def regressaoLogistica(folds, X, Y):\n",
    "    '''\n",
    "        Dado os folds, X e Y.\n",
    "        Então é executado o algoritmo para cada fold.\n",
    "        \n",
    "        Entrada: folds - Folds para serem testados.\n",
    "                 X - Matriz de conjunto de dados.\n",
    "                 Y - Lista com as classes correspondente aos dados da matriz X.\n",
    "        Saida: resultados - Lista de resultados para cada fold.\n",
    "               classes - Lista com as classes baseada na lista Y.\n",
    "    '''\n",
    "    \n",
    "    resultados = []\n",
    "    classes = np.unique(Y)\n",
    "    m, n = X.shape\n",
    "    X = np.column_stack( (np.ones(m),X) ) \n",
    "    MaxIter = 100 \n",
    "\n",
    "    theta = np.zeros(n+1)\n",
    "    result = scipy.optimize.minimize(fun=funcaoCusto, x0=theta, args=(X, Y),  \n",
    "                method='BFGS', jac=True, options={'maxiter': MaxIter, 'disp':True})\n",
    "    theta = result.x\n",
    "\n",
    "    for train_index, test_index in folds:\n",
    "        pred = []\n",
    "        pred = predicao(theta, X[test_index])\n",
    "\n",
    "        cm = get_confusionMatrix(Y[test_index], pred, np.unique(Y))\n",
    "        resultado = relatorioDesempenho(cm, classes, False)\n",
    "\n",
    "        resultados.append(resultado)\n",
    "    \n",
    "    return resultados, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Rotina de testes - RL\n",
    "#----------------------------\n",
    "\n",
    "print('Regressao Logistica em andamento...')\n",
    "start_time = datetime.now()\n",
    "resultados, classes = regressaoLogistica(folds, X, Y)\n",
    "end_time = datetime.now()\n",
    "print('Duração - Regressao Logistica: {}'.format(end_time - start_time))\n",
    "mediaFolds(resultados, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

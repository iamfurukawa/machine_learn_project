{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hotel in', 51),\n",
      " ('room service', 51),\n",
      " ('it is', 51),\n",
      " ('rooms were', 51),\n",
      " ('the room was', 51),\n",
      " ('stayed at the', 51),\n",
      " ('husband and i', 52),\n",
      " ('my husband and i', 52),\n",
      " ('hotel to', 52),\n",
      " ('my wife', 52),\n",
      " ('one of the', 52),\n",
      " ('was the', 53),\n",
      " ('downtown chicago', 53),\n",
      " ('when i', 54),\n",
      " ('place to', 55),\n",
      " ('husband and', 55),\n",
      " ('my husband and', 55),\n",
      " ('with the', 56),\n",
      " ('service was', 59),\n",
      " ('clean and', 59),\n",
      " ('is a', 60),\n",
      " ('all the', 60),\n",
      " ('my room', 60),\n",
      " ('my stay', 61),\n",
      " ('the staff was', 62),\n",
      " ('for the', 62),\n",
      " ('hotel was', 63),\n",
      " ('with a', 63),\n",
      " ('i will', 64),\n",
      " ('the city', 64),\n",
      " ('stay at the', 68),\n",
      " ('i stayed', 68),\n",
      " ('one of', 70),\n",
      " ('we were', 75),\n",
      " ('had a', 75),\n",
      " ('to chicago', 76),\n",
      " ('my husband', 79),\n",
      " ('staff was', 80),\n",
      " ('stayed at', 80),\n",
      " ('in a', 82),\n",
      " ('the best', 82),\n",
      " ('i have', 87),\n",
      " ('from the', 88),\n",
      " ('room was', 88),\n",
      " ('i had', 89),\n",
      " ('to stay', 92),\n",
      " ('was a', 95),\n",
      " ('hotel is', 98),\n",
      " ('a great', 99),\n",
      " ('stay at', 100),\n",
      " ('for a', 107),\n",
      " ('was very', 107),\n",
      " ('i would', 108),\n",
      " ('the rooms', 111),\n",
      " ('the room', 131),\n",
      " ('in chicago', 136),\n",
      " ('to the', 139),\n",
      " ('the staff', 142),\n",
      " ('it was', 156),\n",
      " ('i was', 159),\n",
      " ('this hotel', 191),\n",
      " ('and i', 201),\n",
      " ('and the', 235),\n",
      " ('in the', 238),\n",
      " ('of the', 239),\n",
      " ('the hotel', 242),\n",
      " ('at the', 275)]\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re, operator\n",
    "from pprint import pprint\n",
    "\n",
    "PATH_POSITIVE_TRUTHFUL  = 'op_spam_v1.4/positive/truthful/'\n",
    "PATH_POSITIVE_DECEPTIVE = 'op_spam_v1.4/positive/deceptive/'\n",
    "PATH_NEGATIVE_TRUTHFUL  = 'op_spam_v1.4/negative/truthful/'\n",
    "PATH_NEGATIVE_DECEPTIVE = 'op_spam_v1.4/negative/deceptive/'\n",
    "ALL_PATH = [PATH_POSITIVE_TRUTHFUL, PATH_POSITIVE_DECEPTIVE,\n",
    "           PATH_NEGATIVE_TRUTHFUL, PATH_NEGATIVE_DECEPTIVE]\n",
    "\n",
    "def wordsProcessed(path):\n",
    "    dic = {}\n",
    "    dic2 = {}\n",
    "    for texts in os.listdir(path):\n",
    "        with open(path + texts, 'r', encoding='utf-8') as stream:\n",
    "            text = stream.read()\n",
    "            wordSplited = []\n",
    "            \n",
    "            #Pre-pre processing\n",
    "            wordSplited2 = [word for word in (re.sub(r'[^\\w\\s]+','', text.replace('\\n','')).lower().split(' ')) if word != '']\n",
    "\n",
    "            #N-Gram\n",
    "            for index in range(2, 5):\n",
    "                ngrams = zip(*[wordSplited2[i:] for i in range(index)])\n",
    "                wordSplited += ([\" \".join(ngram) for ngram in ngrams])\n",
    "            \n",
    "            #Creating dictonary for count n-gram words\n",
    "            for word in wordSplited:\n",
    "                if word in dic:\n",
    "                    dic[word] += 1\n",
    "                else:\n",
    "                    dic[word] = 1\n",
    "                    \n",
    "    #Remove values < 2\n",
    "    for key, value in dic.items():\n",
    "        if value > 10:\n",
    "            dic2[key] = value\n",
    "    listWords = [k for k, v in dic2.items()]\n",
    "    dic2 = sorted(dic2.items(), key=operator.itemgetter(1))\n",
    "    \n",
    "    return wordSplited, dic2, listWords\n",
    "\n",
    "posTruthWordSplited, posTruthDic, listWords = wordsProcessed(PATH_POSITIVE_DECEPTIVE)\n",
    "pprint(posTruthDic)\n",
    "print(len(posTruthDic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1.],\n",
       "       [0., 0., 2., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        2., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-28315cb05455>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'X:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;31m#generateFeatures(PATH_POSITIVE_TRUTHFUL)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-28315cb05455>\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0msigma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mddof\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "posTruthWordSplited, posTruthDic, listWords = wordsProcessed(PATH_POSITIVE_TRUTHFUL)\n",
    "matrix = []\n",
    "def generateFeatures(path, example_class):\n",
    "    i = 1\n",
    "    for texts in os.listdir(path):\n",
    "        i+=1\n",
    "        aux = np.zeros(len(listWords) + 1)\n",
    "        with open(path + texts, 'r', encoding='utf-8') as stream:\n",
    "            #Pre-pre processing\n",
    "            text = stream.read()\n",
    "            wordSplited = [word for word in (re.sub(r'[^\\w\\s]+','', text.replace('\\n','')).lower().split(' ')) if word != '']\n",
    "\n",
    "            #N-Gram\n",
    "            for index in range(2, 5):\n",
    "                ngrams = zip(*[wordSplited[i:] for i in range(index)])\n",
    "                wordSplited += ([\" \".join(ngram) for ngram in ngrams])\n",
    "            \n",
    "            for idx, word in enumerate(listWords):\n",
    "                for wordS in wordSplited:\n",
    "                    #print('{} --in-- {} == {}'.format(word, wordS, word == wordS))\n",
    "                    if word == wordS:\n",
    "                        aux[idx] += 1\n",
    "            aux[len(listWords)] = example_class\n",
    "            matrix.append(aux)\n",
    "            #print(len(np.nonzero(aux)[0]))\n",
    "            \n",
    "def normalize(X):\n",
    "    mu = []\n",
    "    sigma = []\n",
    "    m, n = X.shape\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        mu.append( X[ : , i].mean() )\n",
    "        sigma.append (X[ : , i].std(ddof=1) )    \n",
    "\n",
    "    return (X - mu)/sigma\n",
    "\n",
    "def save(m):\n",
    "    with open('./matriz.csv', 'w+', encoding='utf8') as stream:\n",
    "        for word in listWords:\n",
    "            stream.write(word)\n",
    "            stream.write(',')\n",
    "        stream.write('class')\n",
    "        stream.write('\\n')\n",
    "\n",
    "        for x in m:\n",
    "            stream.write('\\n')\n",
    "            for y in x:\n",
    "                stream.write(str(y))\n",
    "                stream.write(',')\n",
    "                \n",
    "generateFeatures(PATH_POSITIVE_DECEPTIVE, 1)\n",
    "save(matrix)\n",
    "df_dataset = pd.read_csv( 'matriz.csv', sep=',', index_col=None)\n",
    "#display(df_dataset.head(n=5))\n",
    "X = df_dataset.iloc[:, 0:-1].values \n",
    "display('X:', X[0:5,:])\n",
    "normalize(X)\n",
    "#generateFeatures(PATH_POSITIVE_TRUTHFUL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " O volume na unidade C nÆo tem nome.\n",
      " O N£mero de S‚rie do Volume ‚ AA8C-336C\n",
      "\n",
      " Pasta de C:\\Users\\Bruno Morii\\Documents\\AM\\Trabalho\\machine_learn_project\n",
      "\n",
      "04/06/2019  22:24    <DIR>          .\n",
      "04/06/2019  22:24    <DIR>          ..\n",
      "04/06/2019  21:29    <DIR>          .ipynb_checkpoints\n",
      "04/06/2019  22:24           129.829 amProject.ipynb\n",
      "04/06/2019  21:05    <DIR>          op_spam_v1.4\n",
      "04/06/2019  21:05         1.172.634 op_spam_v1.4.zip\n",
      "04/06/2019  21:05    <DIR>          op_spam_v1.4_juntos_e_shallow_now\n",
      "04/06/2019  21:05                23 README.md\n",
      "               3 arquivo(s)      1.302.486 bytes\n",
      "               5 pasta(s)   138.192.453.632 bytes dispon¡veis\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

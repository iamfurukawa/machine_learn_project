{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re, operator\n",
    "from pprint import pprint\n",
    "\n",
    "from src.utils import *\n",
    "from src.k_nearest_neighbors import *\n",
    "from src.logistic_regression import *\n",
    "from src.naive_bayes import *\n",
    "from src.neutral_network import *\n",
    "#from src.support_vector_machines import *\n",
    "from src.validation import *\n",
    "\n",
    "PATH_POSITIVE_TRUTHFUL  = 'op_spam_v1.4/positive/truthful/'\n",
    "PATH_POSITIVE_DECEPTIVE = 'op_spam_v1.4/positive/deceptive/'\n",
    "PATH_NEGATIVE_TRUTHFUL  = 'op_spam_v1.4/negative/truthful/'\n",
    "PATH_NEGATIVE_DECEPTIVE = 'op_spam_v1.4/negative/deceptive/'\n",
    "ALL_PATH = [PATH_POSITIVE_TRUTHFUL, PATH_POSITIVE_DECEPTIVE,\n",
    "           PATH_NEGATIVE_TRUTHFUL, PATH_NEGATIVE_DECEPTIVE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\matna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Remoção de Stop Words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download do dicionário de stop wordsgreat\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def stopwords_removal(tokens_list):\n",
    "    new_tokens_list = []\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    new_tokens_list = [token for token in tokens_list if token not in stop_words]\n",
    "    \n",
    "    return new_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram dos tokens\n",
    "def generate_ngram(tokens_list, gram1, gram2):\n",
    "    # N-grams list\n",
    "    allNGrams = []\n",
    "    \n",
    "    #N-Gram\n",
    "    for idx in range(gram1, gram2):\n",
    "        ngrams = zip(*[tokens_list[i:] for i in range(idx)])\n",
    "        allNGrams += ([\" \".join(ngram) for ngram in ngrams])\n",
    "    \n",
    "#     print(allNGrams)\n",
    "    return allNGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remoção de features irrelevantes por threshold\n",
    "def features_removal(features_dic, threshold):\n",
    "    new_features_dic = {}\n",
    "\n",
    "    #Remove values < threshold\n",
    "    for key, value in features_dic.items():\n",
    "        if value > threshold:\n",
    "            new_features_dic[key] = value\n",
    "\n",
    "    new_features_list = [k for k, v in new_features_dic.items()]\n",
    "    new_features_dic = sorted(new_features_dic.items(), key=operator.itemgetter(1))\n",
    "    \n",
    "    return new_features_list, new_features_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "ACABOOOOOOO\n"
     ]
    }
   ],
   "source": [
    "matrix = []\n",
    "Y = []\n",
    "\n",
    "def generateFeatures(token_list, token_per_example, example_class):    \n",
    "    for tokens in token_per_example:\n",
    "        row = np.zeros(len(token_list))\n",
    "        for column, token in enumerate(token_list):\n",
    "            if token in tokens:\n",
    "                row[column] += 1\n",
    "\n",
    "    Y.append(example_class)\n",
    "    matrix.append(row)\n",
    "            \n",
    "generateFeatures(PATH_NEGATIVE_DECEPTIVE, 1)\n",
    "generateFeatures(PATH_NEGATIVE_TRUTHFUL, 0)\n",
    "# generateFeatures(PATH_POSITIVE_DECEPTIVE, 1)\n",
    "# generateFeatures(PATH_POSITIVE_TRUTHFUL, 0)\n",
    "\n",
    "matrix = np.array(matrix)\n",
    "Y = np.array(Y)\n",
    "# matrix_norm, mu, sigma = normalize(matrix)\n",
    "\n",
    "#save(np.column_stack((matrix_norm, matrix[:, -1])), listWords)\n",
    "print('ACABOOOOOOO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2069\n"
     ]
    }
   ],
   "source": [
    "# Tokenização dos textos\n",
    "def pre_processing(paths, stopwords=False, ngram=False, threshold=0):\n",
    "    token_per_example = []\n",
    "    token_dic = {}\n",
    "    dirs =[]\n",
    "    \n",
    "    for p in paths:\n",
    "        text_path = [p + t for t in os.listdir(p)]\n",
    "        dirs += (text_path)\n",
    "    \n",
    "    for texts in dirs:\n",
    "        with open(texts, 'r', encoding='utf-8') as stream:\n",
    "            tokens = []\n",
    "            text = stream.read()\n",
    "\n",
    "            #Pre processing\n",
    "            tokens = [word for word in (re.sub(r'[^\\w\\s]+','', text.replace('\\n','')).lower().split(' ')) if word != '']\n",
    "            \n",
    "            if stopwords:\n",
    "                tokens = stopwords_removal(tokens)\n",
    "            \n",
    "            if ngram:\n",
    "                tokens = generate_ngram(tokens, 1, 3)\n",
    "\n",
    "            token_per_example.append(tokens)\n",
    "            \n",
    "            #Creating dictonary for counting words\n",
    "            for token in tokens:\n",
    "                if token in token_dic:\n",
    "                    token_dic[token] += 1\n",
    "                else:\n",
    "                    token_dic[token] = 1\n",
    "    \n",
    "    token_list, token_dic = features_removal(token_dic, threshold)\n",
    "\n",
    "    generate_features(token_list, token_per_example)\n",
    "    \n",
    "    return token_list, token_dic\n",
    "\n",
    "# posTruthWordSplited, posTruthDic, listWords = wordsProcessed([PATH_POSITIVE_DECEPTIVE, PATH_NEGATIVE_DECEPTIVE], 7)\n",
    "# posTruthWordSplited, posTruthDic, listWords = wordsProcessed([PATH_POSITIVE_DECEPTIVE], 3)\n",
    "posTruthDic, listWords = pre_processing([PATH_NEGATIVE_DECEPTIVE], stopwords=True, ngram=True, threshold=3)\n",
    "\n",
    "#pprint(posTruthDic)\n",
    "print(len(posTruthDic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pre processamento\n",
    "# def pre_processing(paths, stopwords=False, ngram=False, threshold=0):\n",
    "#     tokens, tokens_dic = tokenize(paths)\n",
    "    \n",
    "#     if (stopwords):\n",
    "#         tokens, tokens_dic = stopwords_removal(tokens)\n",
    "    \n",
    "#     if (ngram):\n",
    "#         tokens, tokens_dic = generate_ngram(tokens, 1, 3)\n",
    "    \n",
    "#     tokens, tokens_dic = features_removal(tokens_dic, threshold)\n",
    "    \n",
    "#     return tokens, tokens_dic\n",
    "\n",
    "# # posTruthWordSplited, posTruthDic, listWords = wordsProcessed([PATH_POSITIVE_DECEPTIVE, PATH_NEGATIVE_DECEPTIVE], 7)\n",
    "# # posTruthWordSplited, posTruthDic, listWords = wordsProcessed([PATH_POSITIVE_DECEPTIVE], 3)\n",
    "# posTruthDic, listWords = pre_processing([PATH_NEGATIVE_DECEPTIVE])\n",
    "\n",
    "# #pprint(posTruthDic)\n",
    "# print(len(posTruthDic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(matrix_norm)\n",
    "if matrix_norm.shape[1] != len(posTruthDic):\n",
    "    raise Exception('Tamanhos diferentes!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = matrix_norm[400:720]\n",
    "# b = matrix_norm[0:320]\n",
    "# X2 = np.append(a,b, axis=0)\n",
    "\n",
    "# a = Y[400:720]\n",
    "# b = Y[0:320]\n",
    "# Y2 = np.append(a,b, axis=0)\n",
    "\n",
    "# a = matrix_norm[720:800]\n",
    "# b = matrix_norm[320:400]\n",
    "# X_val = np.append(a,b, axis=0)\n",
    "\n",
    "# a = Y[720:800]\n",
    "# b = Y[320:400]\n",
    "# Y_val = np.append(a,b, axis=0)\n",
    "\n",
    "#custo, gamma = svm(X2, Y2, X_val, Y_val)\n",
    "#print(custo, '\\t', gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = knn(X_val[81], X2, Y2, 5)\n",
    "# #print(Y_val[81], ' ',y,' ', ind_viz)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = stratified_kfolds(Y, 5, np.unique(Y))\n",
    "\n",
    "for train_index, test_index in folds:\n",
    "    print('Train: ',len(train_index))\n",
    "    print('Test: ',len(test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# resultados = []\n",
    "# for x in range(0, 5):\n",
    "#     Y_resultado = np.array([random.randint(0, 1) for i in range(0, 160)])\n",
    "#     Y_classes = np.array([random.randint(0, 1) for i in range(0, 160)])\n",
    "#     classes = [0, 1]\n",
    "\n",
    "#     cm = get_confusionMatrix(Y_classes, Y_resultado, classes)\n",
    "    \n",
    "    \n",
    "#     resultado = relatorioDesempenho(cm, classes, False)\n",
    "#     resultados.append(resultado)\n",
    "\n",
    "# mediaFolds(resultados, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAIVE BAYES\n",
    "\n",
    "resultados = []\n",
    "classes = np.unique(Y)\n",
    "\n",
    "for train_index, test_index in folds:\n",
    "    # Treinamento\n",
    "    probsPos, probsNeg = calcularProbabilidades(matrix[train_index], Y[train_index])\n",
    "    \n",
    "    # Classificação\n",
    "    pred = []\n",
    "    for x in matrix[test_index]:\n",
    "        probPos, probNeg = classificacao(x, probsPos, probsNeg, sum(Y == 1)/len(Y), sum(Y == 0)/len(Y))\n",
    "        if (probPos >= probNeg):\n",
    "            pred.append(1)\n",
    "        else:\n",
    "            pred.append(0)\n",
    "\n",
    "    cm = get_confusionMatrix(Y[test_index], pred, np.unique(Y))\n",
    "    resultado = relatorioDesempenho(cm, classes, False)\n",
    "    \n",
    "    resultados.append(resultado)\n",
    "\n",
    "mediaFolds(resultados, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nop 0\n",
      "yep 1\n",
      "yep 2\n",
      "\n",
      "\n",
      "nop 0\n",
      "yep 1\n",
      "nop 2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print('s' in ['a','b','c'])\n",
    "token_per_example = [['e','b','c'], ['a','b','c']]\n",
    "\n",
    "#tokens = ['a','b','c']\n",
    "token_list = ['s', 'c', 'e']\n",
    "\n",
    "for tokens in token_per_example:\n",
    "    for idx, token in enumerate(token_list):\n",
    "        if token in tokens:\n",
    "            print('yep', idx)\n",
    "        else:\n",
    "            print('nop', idx)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

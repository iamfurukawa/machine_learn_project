{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re, operator\n",
    "from pprint import pprint\n",
    "\n",
    "from src.utils import *\n",
    "from src.k_nearest_neighbors import *\n",
    "from src.logistic_regression import *\n",
    "from src.naive_bayes import *\n",
    "from src.neutral_network import *\n",
    "#from src.support_vector_machines import *\n",
    "from src.validation import *\n",
    "\n",
    "PATH_POSITIVE_TRUTHFUL  = 'op_spam_v1.4/positive/truthful/'\n",
    "PATH_POSITIVE_DECEPTIVE = 'op_spam_v1.4/positive/deceptive/'\n",
    "PATH_NEGATIVE_TRUTHFUL  = 'op_spam_v1.4/negative/truthful/'\n",
    "PATH_NEGATIVE_DECEPTIVE = 'op_spam_v1.4/negative/deceptive/'\n",
    "ALL_PATH = [PATH_POSITIVE_TRUTHFUL, PATH_POSITIVE_DECEPTIVE,\n",
    "           PATH_NEGATIVE_TRUTHFUL, PATH_NEGATIVE_DECEPTIVE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\matna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Remoção de Stop Words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download do dicionário de stop wordsgreat\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def stopwords_removal(tokens_list):\n",
    "    new_tokens_list = []\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    new_tokens_list = [token for token in tokens_list if token not in stop_words]\n",
    "    \n",
    "    return new_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram dos tokens\n",
    "def generate_ngram(tokens_list, gram1, gram2):\n",
    "    # N-grams list\n",
    "    allNGrams = []\n",
    "    \n",
    "    #N-Gram\n",
    "    for idx in range(gram1, gram2):\n",
    "        ngrams = zip(*[tokens_list[i:] for i in range(idx)])\n",
    "        allNGrams += ([\" \".join(ngram) for ngram in ngrams])\n",
    "    \n",
    "#     print(allNGrams)\n",
    "    return allNGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remoção de features irrelevantes por threshold\n",
    "def features_removal(features_dic, threshold):\n",
    "    new_features_dic = {}\n",
    "\n",
    "    #Remove values < threshold\n",
    "    for key, value in features_dic.items():\n",
    "        if value > threshold:\n",
    "            new_features_dic[key] = value\n",
    "\n",
    "    new_features_list = [k for k, v in new_features_dic.items()]\n",
    "    new_features_dic = sorted(new_features_dic.items(), key=operator.itemgetter(1))\n",
    "    \n",
    "    return new_features_list, new_features_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pre processamento\n",
    "# def pre_processing(paths, stopwords=False, ngram=False, threshold=0):\n",
    "#     tokens, tokens_dic = tokenize(paths)\n",
    "    \n",
    "#     if (stopwords):\n",
    "#         tokens, tokens_dic = stopwords_removal(tokens)\n",
    "    \n",
    "#     if (ngram):\n",
    "#         tokens, tokens_dic = generate_ngram(tokens, 1, 3)\n",
    "    \n",
    "#     tokens, tokens_dic = features_removal(tokens_dic, threshold)\n",
    "    \n",
    "#     return tokens, tokens_dic\n",
    "\n",
    "# # posTruthWordSplited, posTruthDic, listWords = wordsProcessed([PATH_POSITIVE_DECEPTIVE, PATH_NEGATIVE_DECEPTIVE], 7)\n",
    "# # posTruthWordSplited, posTruthDic, listWords = wordsProcessed([PATH_POSITIVE_DECEPTIVE], 3)\n",
    "# posTruthDic, listWords = pre_processing([PATH_NEGATIVE_DECEPTIVE])\n",
    "\n",
    "# #pprint(posTruthDic)\n",
    "# print(len(posTruthDic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4966\n"
     ]
    }
   ],
   "source": [
    "# Tokenização dos textos\n",
    "def pre_processing(paths, stopwords=False, ngram=False, threshold=0):\n",
    "    token_dic = {}\n",
    "    dirs =[]\n",
    "    \n",
    "    for p in paths:\n",
    "        text_path = [p + t for t in os.listdir(p)]\n",
    "        dirs += (text_path)\n",
    "    \n",
    "    for texts in dirs:\n",
    "        with open(texts, 'r', encoding='utf-8') as stream:\n",
    "            tokens = []\n",
    "            text = stream.read()\n",
    "\n",
    "            #Pre processing\n",
    "            tokens = [word for word in (re.sub(r'[^\\w\\s]+','', text.replace('\\n','')).lower().split(' ')) if word != '']\n",
    "            \n",
    "            if stopwords:\n",
    "                tokens = stopwords_removal(tokens)\n",
    "            \n",
    "            if ngram:\n",
    "                tokens = generate_ngram(tokens, 1, 3)    \n",
    "\n",
    "            #Creating dictonary for counting words\n",
    "            for token in tokens:\n",
    "                if token in token_dic:\n",
    "                    token_dic[token] += 1\n",
    "                else:\n",
    "                    token_dic[token] = 1\n",
    "    \n",
    "    token_list, token_dic = features_removal(token_dic, threshold)\n",
    "#     pprint(token_dic)\n",
    "    return token_list, token_dic\n",
    "\n",
    "# posTruthWordSplited, posTruthDic, listWords = wordsProcessed([PATH_POSITIVE_DECEPTIVE, PATH_NEGATIVE_DECEPTIVE], 7)\n",
    "# posTruthWordSplited, posTruthDic, listWords = wordsProcessed([PATH_POSITIVE_DECEPTIVE], 3)\n",
    "posTruthDic, listWords = pre_processing([PATH_NEGATIVE_DECEPTIVE], stopwords=True, threshold=3)\n",
    "\n",
    "#pprint(posTruthDic)\n",
    "print(len(posTruthDic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = []\n",
    "Y = []\n",
    "\n",
    "def generateFeatures(path, example_class):\n",
    "    i = 1\n",
    "    for texts in os.listdir(path):\n",
    "        i+=1\n",
    "        print(i)\n",
    "        aux = np.zeros(len(listWords))\n",
    "        with open(path + texts, 'r', encoding='utf-8') as stream:\n",
    "            #Pre-pre processing\n",
    "            text = stream.read()\n",
    "            allNgrams = []\n",
    "            wordSplited = [word for word in (re.sub(r'[^\\w\\s]+','', text.replace('\\n','')).lower().split(' ')) if word != '']\n",
    "\n",
    "            stop_words = stopwords.words('english')\n",
    "            wordSplited = [word for word in wordSplited if word not in stop_words]\n",
    "            \n",
    "            #N-Gram\n",
    "            for index in range(2, 4):\n",
    "                ngrams = zip(*[wordSplited[i:] for i in range(index)])\n",
    "                allNgrams += ([\" \".join(ngram) for ngram in ngrams])\n",
    "            \n",
    "            for idx, word in enumerate(listWords):\n",
    "                for ngram in allNgrams:\n",
    "                    if word == ngram:\n",
    "                        aux[idx] += 1\n",
    "\n",
    "            Y.append(example_class)\n",
    "            matrix.append(aux)\n",
    "            \n",
    "generateFeatures(PATH_NEGATIVE_DECEPTIVE, 1)\n",
    "generateFeatures(PATH_NEGATIVE_TRUTHFUL, 0)\n",
    "generateFeatures(PATH_POSITIVE_DECEPTIVE, 1)\n",
    "generateFeatures(PATH_POSITIVE_TRUTHFUL, 0)\n",
    "\n",
    "matrix = np.array(matrix)\n",
    "Y = np.array(Y)\n",
    "matrix_norm, mu, sigma = normalize(matrix)\n",
    "\n",
    "#save(np.column_stack((matrix_norm, matrix[:, -1])), listWords)\n",
    "print('ACABOOOOOOO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if matrix_norm.shape[1] != len(posTruthDic):\n",
    "    raise Exception('Tamanhos diferentes!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = matrix_norm[400:720]\n",
    "# b = matrix_norm[0:320]\n",
    "# X2 = np.append(a,b, axis=0)\n",
    "\n",
    "# a = Y[400:720]\n",
    "# b = Y[0:320]\n",
    "# Y2 = np.append(a,b, axis=0)\n",
    "\n",
    "# a = matrix_norm[720:800]\n",
    "# b = matrix_norm[320:400]\n",
    "# X_val = np.append(a,b, axis=0)\n",
    "\n",
    "# a = Y[720:800]\n",
    "# b = Y[320:400]\n",
    "# Y_val = np.append(a,b, axis=0)\n",
    "\n",
    "#custo, gamma = svm(X2, Y2, X_val, Y_val)\n",
    "#print(custo, '\\t', gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = knn(X_val[81], X2, Y2, 5)\n",
    "# #print(Y_val[81], ' ',y,' ', ind_viz)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = stratified_kfolds(Y, 5, np.unique(Y))\n",
    "\n",
    "for train_index, test_index in folds:\n",
    "    print('Train: ',len(train_index))\n",
    "    print('Test: ',len(test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# resultados = []\n",
    "# for x in range(0, 5):\n",
    "#     Y_resultado = np.array([random.randint(0, 1) for i in range(0, 160)])\n",
    "#     Y_classes = np.array([random.randint(0, 1) for i in range(0, 160)])\n",
    "#     classes = [0, 1]\n",
    "\n",
    "#     cm = get_confusionMatrix(Y_classes, Y_resultado, classes)\n",
    "    \n",
    "    \n",
    "#     resultado = relatorioDesempenho(cm, classes, False)\n",
    "#     resultados.append(resultado)\n",
    "\n",
    "# mediaFolds(resultados, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAIVE BAYES\n",
    "\n",
    "resultados = []\n",
    "classes = np.unique(Y)\n",
    "\n",
    "for train_index, test_index in folds:\n",
    "    # Treinamento\n",
    "    probsPos, probsNeg = calcularProbabilidades(matrix[train_index], Y[train_index])\n",
    "    \n",
    "    # Classificação\n",
    "    pred = []\n",
    "    for x in matrix[test_index]:\n",
    "        probPos, probNeg = classificacao(x, probsPos, probsNeg, sum(Y == 1)/len(Y), sum(Y == 0)/len(Y))\n",
    "        if (probPos >= probNeg):\n",
    "            pred.append(1)\n",
    "        else:\n",
    "            pred.append(0)\n",
    "\n",
    "    cm = get_confusionMatrix(Y[test_index], pred, np.unique(Y))\n",
    "    resultado = relatorioDesempenho(cm, classes, False)\n",
    "    \n",
    "    resultados.append(resultado)\n",
    "\n",
    "mediaFolds(resultados, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

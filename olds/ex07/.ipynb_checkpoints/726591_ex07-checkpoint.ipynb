{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"figs/LogoUFSCar.jpg\" alt=\"Logo UFScar\" width=\"110\" align=\"left\"/>  <br/> <center>Universidade Federal de São Carlos (UFSCar)<br/><font size=\"4\"> Departamento de Computação, campus Sorocaba</center></font>\n",
    "</p>\n",
    "\n",
    "<br/>\n",
    "<font size=\"4\"><center><b>Disciplina: Aprendizado de Máquina</b></center></font>\n",
    "  \n",
    "<font size=\"3\"><center>Prof. Dr. Tiago A. Almeida</center></font>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<center><i><b>\n",
    "Atenção: não são autorizadas cópias, divulgações ou qualquer tipo de uso deste material sem o consentimento prévio dos autores.\n",
    "</center></i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Exercício - Redes Neurais Artificiais </center>\n",
    "\n",
    "Neste exercício, você irá implementar uma rede neural artificial com *backpropagation* que será aplicada na tarefa de reconhecimento de dígitos manuscritos. Antes de iniciar, é fortemente recomendado que você revise o material apresentado em aula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O problema\n",
    "\n",
    "Você foi contratado por uma grande empresa para fazer a identificação correta e automática de quais dígitos estão presentes em um conjunto de imagens. Essas imagens têm dimensão de 20 x 20 pixels, onde cada pixel é representado por um ponto flutuante que indica a intensidade de tons de cinza naquela região.\n",
    "\n",
    "Sabe-se que a aplicação de redes neurais utilizando o algoritmo de *backpropagation* neste tipo de problema obtêm resultados satisfatórios. Assim, seu desafio é implementar tal algoritmo e encontrar os pesos ótimos para que a rede seja capaz de identificar automaticamente os dígitos contidos nas imagens.\n",
    "\n",
    "<center>\n",
    "<div style=\"padding: 0px; float: center;\">\n",
    "    <img src=\"figs/digitos.png\"  style=\"height:400px;\"/> \n",
    "    <center><em>Figura 1. Amostras do conjunto de dados.</em></center>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Carregando e visualizando os dados\n",
    "\n",
    "Nessa etapa, você irá completar a função para plotar os dados. O conjunto que você utilizará será de digitos manuscritos (Figura 1).\n",
    "\n",
    "Cada imagem tem dimensão de 20 x 20 pixels e cada pixel é representado por um ponto flutuante com a intensidade do tom de cinza naquela região. Deste modo, cada amostra é representada pelo desdobramento dos pixels em um vetor com 400 dimensões.\n",
    "\n",
    "O conjunto de dados contém 5.000 amostras, sendo cada amostra representada por um vetor com 400 dimensões. Portanto, o conjunto é representado por uma matriz [5000,400].\n",
    "\n",
    "A segunda parte do conjunto de dados é um vetor $y$ com 5.000 dimensões, o qual contém os rótulos para cada amostra da base de treino. Imagens contendo dígitos de 1 a 9 recebem, respectivamente, classes de 1 a 9, enquanto imagens contendo o dígito 0 são rotuladas como 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro, vamos carregar os dados do arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np  # importa a biblioteca usada para trabalhar com vetores e matrizes\n",
    "import pandas as pd # importa a biblioteca usada para trabalhar com dataframes (dados em formato de tabela) e análise de dados\n",
    "\n",
    "# importa o arquivo e guarda em um dataframe do Pandas\n",
    "df_dataset = pd.read_csv( 'dados.csv', sep=',', header=None) \n",
    "\n",
    "print('Dados carregados com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos guardar os valores dentro de um array X e as classes dentro de um vetor Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y: [10 10 10 10 10]\n",
      "\n",
      "Dimensao de X:  (5000, 400)\n",
      "\n",
      "Dimensao de Y:  (5000,)\n",
      "\n",
      "Classes do problema:  [ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "# Pega os valores das n-1 primeiras colunas e guarda em uma matrix X\n",
    "X = df_dataset.iloc[:, 0:-1].values \n",
    "\n",
    "# Pega os valores da ultima coluna e guarda em um vetor Y\n",
    "Y = df_dataset.iloc[:, -1].values \n",
    "\n",
    "# Imprime as 5 primeiras linhas da matriz X\n",
    "display('X:', X[0:5,:])\n",
    "\n",
    "# Imprime os 5 primeiros valores de Y\n",
    "print('Y:', Y[0:5])\n",
    "\n",
    "print('\\nDimensao de X: ', X.shape)\n",
    "\n",
    "print('\\nDimensao de Y: ', Y.shape)\n",
    "\n",
    "print('\\nClasses do problema: ', np.unique(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos plotar aleatoriamente 100 amostras da base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 700x700 with 100 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualizaDados(X):\n",
    "    example_width = int(round(np.sqrt(X.shape[1])) )\n",
    "\n",
    "    # Calcula numero de linhas e colunas\n",
    "    m, n = X.shape\n",
    "    example_height = int(n / example_width)\n",
    "\n",
    "    # Calcula numero de itens que serao exibidos\n",
    "    display_rows = int(np.floor(np.sqrt(m)))\n",
    "    display_cols = int(np.ceil(m / display_rows))\n",
    "\n",
    "    fig, axs = plt.subplots(display_rows,display_cols, figsize=(7, 7))\n",
    "                        \n",
    "    for ax, i in zip(axs.ravel(), range( X.shape[0] )):\n",
    "    \n",
    "        new_X = np.reshape( np.ravel(X[i,:]), (example_width, example_height) )\n",
    "\n",
    "        ax.imshow(new_X.T, cmap='gray'); \n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "idx_perm = np.random.permutation( range(X.shape[0]) )\n",
    "visualizaDados( X[idx_perm[0:100],:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Carregando os parâmetros\n",
    "\n",
    "A rede neural proposta para este exercício tem 3 camadas: uma camada de entrada, uma camada oculta e uma camada de saída  (Figura 2). É importante lembrar que a camada de entrada possui 400 neurônios devido ao desdobramento dos pixels das amostras em vetores com 400 dimensões (sem considerar o *bias*, sempre +1).\n",
    "\n",
    "<center>\n",
    "<div style=\"padding: 5px; float: center;\">\n",
    "    <img src=\"figs/nn.png\"  style=\"height:350px;\"/> \n",
    "    <center><em>Figura 2. Arquitetura da rede neural.</em></center>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "Inicialmente, você terá acesso aos parâmetros ($\\Theta^{(1)}$, $\\Theta^{(2)}$) de uma rede neural já treinada, armazenados nos arquivos **pesos_Theta1.csv** e **pesos_Theta2.csv**. Tais parâmetros têm dimensões condizentes com uma rede neural com 25 neurônios na camada intermediária e 10 neurônios na camada de saída (correspondente às dez possíveis classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos carregar os pesos pré-treinados para a rede neural e inicializar os parâmetros mais importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carregando parametros salvos da rede neural...\n",
      "\n",
      "Pesos carregados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# parametros a serem utilizados neste exercicio\n",
    "input_layer_size  = 400  # 20x20 dimensao das imagens de entrada\n",
    "hidden_layer_size = 25   # 25 neuronios na camada oculta\n",
    "num_labels = 10          # 10 rotulos, de 1 a 10  \n",
    "                         #  (observe que a classe \"0\" recebe o rotulo 10)\n",
    "    \n",
    "print('\\nCarregando parametros salvos da rede neural...\\n')\n",
    "\n",
    "# carregando os pesos da camada 1\n",
    "Theta1 = pd.read_csv('pesos_Theta1.csv', sep=',', header=None).values\n",
    "\n",
    "# carregando os pesos da camada 2\n",
    "Theta2 = pd.read_csv('pesos_Theta2.csv', sep=',', header=None).values\n",
    "\n",
    "# concatena os pesos em um único vetor\n",
    "nn_params = np.concatenate([np.ravel(Theta1), np.ravel(Theta2)])\n",
    "\n",
    "print('Pesos carregados com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Calcula o custo (*Feedforward*)\n",
    "\n",
    "Você precisará implementar a função de custo e gradiente para a rede neural. A função de custo (sem regularização) é descrita a seguir.\n",
    "\n",
    "$$J(\\Theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[-y_k^{(i)} \\log\\left( \\left(h_\\Theta(x^{(i)})\\right)_k \\right) - \\left(1 - y_k^{(i)}\\right) \\log \\left( 1 - \\left(h_\\Theta(x^{(i)} )\\right)_k \\right)\\right]$$\n",
    "\n",
    "Na função $J$, $h_\\Theta(x^{(i)})$ é computado conforme representado na Figura 2. A constante $K$ representa o número de classes. Assim, $h_\\Theta(x^{(i)})_k$ = $a^{(3)}_k$ corresponde à ativação (valor de saída) da $k$-ésima unidade de saída. Também, é importante lembrar que o vetor de saída precisa ser criado a partir da classe original, tornando-se compatível com a rede neural, ou seja, espera-se vetores com 10 posições contendo 1 para o elemento referente à classe esperada e 0 nos demais elementos. Por exemplo, seja 5 o rótulo de determinada amostra, o vetor $Y$ correspondente terá 1 na posição $y_5$ e 0 nas demais posições.\n",
    "\n",
    "Você precisará implementar o algoritmo *feedfoward* para calcular $h_\\Theta(x^{(i)})$ para cada amostra $i$ e somar o custo de todas as amostras. Seu código deverá ser flexível para funcionar com conjuntos de dados de qualquer tamanho e qualquer quantidade de classes, considerando $K \\geq 3$.\n",
    "\n",
    "Nesta fase, implemente a função de custo sem regularização para facilitar a análise. Posteriormente, você implementará o custo regularizado.\n",
    "\n",
    "Antes de implementar a função de custo, você precisará da função sigmoidal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid de 0 = 0.500000\n",
      "sigmoid de 10 = 0.999955\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Calcula a função sigmoidal  \n",
    "    \"\"\"\n",
    "\n",
    "    z = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return z\n",
    "\n",
    "# testando a função sigmoidal\n",
    "z = sigmoid(0)\n",
    "print('sigmoid de 0 = %1.6f' %(z))\n",
    "\n",
    "z = sigmoid(10)\n",
    "print('sigmoid de 10 = %1.6f' %(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete a função que será usada para calcular o custo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Funcao de custo sem regularizacao ...\n",
      "\n",
      "Custo com os parametros (carregados do arquivo): 0.287629 \n",
      "\n",
      "(este valor deve ser proximo de 0.287629)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def funcaoCusto(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y):\n",
    "    '''\n",
    "    Implementa a funcao de custo para a rede neural com duas camadas\n",
    "    voltada para tarefa de classificacao\n",
    "    \n",
    "    Calcula o custo e gradiente da rede neural. \n",
    "    Os parametros da rede neural sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    input_layer_size - tamanho da camada de entrada\n",
    "    hidden_layer_size - tamanho da camada oculta\n",
    "    num_labels - numero de classes possiveis\n",
    "    \n",
    "    O vetor grad de retorno contem todas as derivadas parciais\n",
    "    da rede neural.\n",
    "    '''\n",
    "\n",
    "    # Extrai os parametros de nn_params e alimenta as variaveis Theta1 e Theta2.\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # A variavel a seguir precisa ser retornada corretamente\n",
    "    J = 0;\n",
    "    \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #               acompanhando os seguintes passos.\n",
    "    #\n",
    "    # (1): Lembre-se de transformar os rotulos Y em vetores com 10 posicoes,\n",
    "    #         onde tera zero em todas posicoes exceto na posicao do rotulo\n",
    "    #\n",
    "    # (2): Execute a etapa de feedforward e coloque o custo na variavel J.\n",
    "    #\n",
    "    eps = 1e-15\n",
    "    y_rotulo = np.zeros((len(y),num_labels),dtype=int)\n",
    "    for i in range(len(y)) :\n",
    "        for j in range(num_labels) :\n",
    "            if j == y[i]-1 :\n",
    "                y_rotulo[i][j] = 1\n",
    "            else :\n",
    "                y_rotulo[i][j] = 0\n",
    "    \n",
    "    z2 = np.insert(X,0,1,axis = 1).dot(Theta1.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    #display(y[0:30])\n",
    "    \n",
    "    #a1 = (((-y) * np.log(z1).T) - ((1-y) * (np.log(1-z1).T) ))\n",
    "    #a1 = a1.T\n",
    "    \n",
    "    z3 = np.insert(a2,0,1,axis = 1).dot(Theta2.T)\n",
    "    a3 = sigmoid(z3) \n",
    "    \n",
    "    #a3 = a3.T\n",
    "    #print(sum(sum(a2))/m)\n",
    "    \n",
    "    J = (1/m) * sum(np.sum(((-y_rotulo) * np.log(a3 + eps)) - ((1-y_rotulo) * (np.log(1-a3 + eps) )),axis=1)) \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "print('\\nFuncao de custo sem regularizacao ...\\n')\n",
    "\n",
    "J = funcaoCusto(nn_params, input_layer_size, hidden_layer_size, num_labels, X, Y)\n",
    "\n",
    "print('Custo com os parametros (carregados do arquivo): %1.6f ' %J)\n",
    "print('\\n(este valor deve ser proximo de 0.287629)\\n')\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Regularização\n",
    "\n",
    "A função de custo com regularização é descrita a seguir.\n",
    "\n",
    "$$J(\\Theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[-y_k^{(i)} \\log\\left( \\left(h_\\Theta(x^{(i)})\\right)_k \\right) - \\left(1 - y_k^{(i)}\\right) \\log \\left( 1 - \\left(h_\\Theta(x^{(i)} )\\right)_k \\right)\\right]$$\n",
    "\n",
    "$$ + \\frac{\\lambda}{2m} \\left[\\sum_{j=1}^{25} \\sum_{k=1}^{400} \\left(\\Theta^{(1)}_{j,k}\\right)^2 + \\sum_{j=1}^{10} \\sum_{k=1}^{25} \\left(\\Theta^{(2)}_{j,k}\\right)^2\\right]$$\n",
    "\n",
    "Você pode assumir que a rede neural terá 3 camadas - uma camada de entrada, uma camada oculta e uma camada de saída. No entanto, seu código deverá ser flexível para suportar qualquer quantidade de neurônios em cada uma dessas camadas. Embora a função $J$ descrita anteriormente contenha números fixos para $\\Theta^{(1)}$ e $\\Theta^{(2)}$, seu código deverá funcionar para outros tamanhos.\n",
    "\n",
    "Também, é importante que a regularização não seja aplicada a termos relacionados aos *bias*. Neste contexto, estes termos estão na primeira coluna de cada matriz $\\Theta^{(1)}$ e $\\Theta^{(2)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, complete a nova função de custo aplicando regularização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcaoCusto_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, vLambda):\n",
    "    '''\n",
    "    Implementa a funcao de custo para a rede neural com duas camadas\n",
    "    voltada para tarefa de classificacao\n",
    "    \n",
    "    Calcula o custo e gradiente da rede neural. \n",
    "    Os parametros da rede neural sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    input_layer_size - tamanho da camada de entrada\n",
    "    hidden_layer_size - tamanho da camada oculta\n",
    "    num_labels - numero de classes possiveis\n",
    "    lambda - parametro de regularizacao\n",
    "    \n",
    "    O vetor grad de retorno contem todas as derivadas parciais\n",
    "    da rede neural.\n",
    "    '''\n",
    "\n",
    "    # Extrai os parametros de nn_params e alimenta as variaveis Theta1 e Theta2.\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # A variavel a seguir precisa ser retornada corretamente\n",
    "    J = 0;\n",
    "    \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #               acompanhando os seguintes passos.\n",
    "    #\n",
    "    # (1): Lembre-se de transformar os rotulos Y em vetores com 10 posicoes,\n",
    "    #         onde tera zero em todas posicoes exceto na posicao do rotulo\n",
    "    #\n",
    "    # (2): Execute a etapa de feedforward e coloque o custo na variavel J.\n",
    "    #\n",
    "    #\n",
    "    # (3): Implemente a regularização na função de custo.\n",
    "    #\n",
    "\n",
    "    eps = 1e-15\n",
    "    y_rotulo = np.zeros((len(y),num_labels),dtype=int)\n",
    "    for i in range(len(y)) :\n",
    "        for j in range(num_labels) :\n",
    "            if j == y[i]-1 :\n",
    "                y_rotulo[i][j] = 1\n",
    "            else :\n",
    "                y_rotulo[i][j] = 0\n",
    "    \n",
    "    z2 = np.insert(X,0,1,axis = 1).dot(Theta1.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = np.insert(a2,0,1,axis = 1).dot(Theta2.T)\n",
    "    a3 = sigmoid(z3) \n",
    "    \n",
    "    reg = sum(np.sum(np.power(Theta1[:,1:],2),axis=1)) + sum(np.sum(np.power(Theta2[:,1:],2),axis=1))\n",
    "    reg = (vLambda/(2 * m)) * (reg)      \n",
    "    \n",
    "    J = (1/m) * sum(np.sum(((-y_rotulo) * np.log(a3 + eps)) - ((1-y_rotulo) * (np.log(1-a3 + eps) )),axis=1)) + reg\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checando a funcao de custo (c/ regularizacao) ... \n",
      "\n",
      "Custo com os parametros (carregados do arquivo): 0.383770 \n",
      "\n",
      "(este valor deve ser proximo de 0.383770)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nChecando a funcao de custo (c/ regularizacao) ... \\n')\n",
    "\n",
    "# Parametro de regularizacao dos pesos (aqui sera igual a 1).\n",
    "vLambda = 1;\n",
    "\n",
    "J = funcaoCusto_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, X, Y, vLambda)\n",
    "\n",
    "print('Custo com os parametros (carregados do arquivo): %1.6f ' %J)\n",
    "print('\\n(este valor deve ser proximo de 0.383770)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 5: Inicialização dos parâmetros\n",
    "\n",
    "Nesta parte, começa a implementação das duas camadas da rede neural para classificação dos dígitos manuscritos. Os pesos são inicializados aleatoriamente. Mas, para que toda a execução gere o mesmo resultado, vamos usar uma semente para a função de geração de números aleatórios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inicializando parametros da rede neural...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def inicializaPesosAleatorios(L_in, L_out, randomSeed = None):\n",
    "    '''\n",
    "    Inicializa aleatoriamente os pesos de uma camada usando \n",
    "    L_in (conexoes de entrada) e L_out (conexoes de saida).\n",
    "\n",
    "    W sera definido como uma matriz de dimensoes [L_out, 1 + L_in]\n",
    "    visto que devera armazenar os termos para \"bias\".\n",
    "    \n",
    "    randomSeed: indica a semente para o gerador aleatorio\n",
    "    '''\n",
    "\n",
    "    epsilon_init = 0.12\n",
    "    \n",
    "    # se for fornecida uma semente para o gerador aleatorio\n",
    "    if randomSeed is not None:\n",
    "        W = np.random.RandomState(randomSeed).rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "        \n",
    "    # se nao for fornecida uma semente para o gerador aleatorio\n",
    "    else:\n",
    "        W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "        \n",
    "    return W\n",
    "\n",
    "\n",
    "print('\\nInicializando parametros da rede neural...\\n')\n",
    "    \n",
    "initial_Theta1 = inicializaPesosAleatorios(input_layer_size, hidden_layer_size, randomSeed = 10)\n",
    "initial_Theta2 = inicializaPesosAleatorios(hidden_layer_size, num_labels, randomSeed = 20)\n",
    "\n",
    "# junta os pesos iniciais em um unico vetor\n",
    "initial_rna_params = np.concatenate([np.ravel(initial_Theta1), np.ravel(initial_Theta2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 6: Backpropagation\n",
    "\n",
    "Nesta parte do exercício, você implementará o algoritmo de *backpropagation* responsável por calcular o gradiente para a função de custo da rede neural. Terminada a implementação do cálculo do gradiente, você poderá treinar a rede neural minimizando a função de custo $J(\\Theta)$ usando um otimizador avançado, como a funcao `minimize` do ScyPy.\n",
    "\n",
    "Primeiro, você precisará implementar o gradiente para a rede neural sem regularização. Após ter verificado que o cálculo do gradiente está correto, você implementará o gradiente para a rede neural com regularização.\n",
    "\n",
    "Você deverá começar pela implementação do gradiente da sigmóide, o qual pode ser calculado utilizando a equação:\n",
    "\n",
    "$$ g'(z) = \\frac{d}{dz}g(z) = g(z)(1-g(z)), $$\n",
    "\n",
    "sendo que\n",
    "\n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}}. $$\n",
    "\n",
    "Ao completar, teste diferentes valores para a função `sigmoidGradient`. Para valores grandes de *z* (tanto positivo, quanto negativo), o resultado deve ser próximo a zero. Quando $z = 0$, o resultado deve ser exatamente 0,25. A função deve funcionar com vetores e matrizes também. No caso de matrizes, a função deve calcular o gradiente para cada elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando o gradiente da sigmoide...\n",
      "\n",
      "Gradiente da sigmoide avaliado em [1 -0.5 0 0.5 1]:\n",
      "\n",
      "[0.19661193 0.23500371 0.25       0.23500371 0.19661193]\n",
      "\n",
      "Se sua implementacao estiver correta, o gradiente da sigmoide sera:\n",
      "[0.19661193 0.23500371 0.25 0.23500371 0.19661193]:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sigmoidGradient(z):\n",
    "    '''\n",
    "    Retorna o gradiente da funcao sigmoidal para z \n",
    "    \n",
    "    Calcula o gradiente da funcao sigmoidal\n",
    "    para z. A funcao deve funcionar independente se z for matriz ou vetor.\n",
    "    Nestes casos,  o gradiente deve ser calculado para cada elemento.\n",
    "    '''\n",
    "    \n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Calcula o gradiente da funcao sigmoidal para \n",
    "    #           cada valor de z (seja z matriz, escalar ou vetor).\n",
    "    #\n",
    "\n",
    "    g = (1/(1+np.exp(-z))) * (1- (1/(1+np.exp(-z))))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    ##########################################################################\n",
    "\n",
    "    return g\n",
    "\n",
    "print('\\nAvaliando o gradiente da sigmoide...\\n')\n",
    "\n",
    "g = sigmoidGradient(np.array([1,-0.5, 0, 0.5, 1]))\n",
    "print('Gradiente da sigmoide avaliado em [1 -0.5 0 0.5 1]:\\n')\n",
    "print(g)\n",
    "\n",
    "print('\\nSe sua implementacao estiver correta, o gradiente da sigmoide sera:')\n",
    "print('[0.19661193 0.23500371 0.25 0.23500371 0.19661193]:\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo do algoritmo *backpropagation* é encontrar a \"parcela de responsabilidade\" que cada neurônio da rede neural teve com o erro gerado na saída. Dada uma amostra de treino ($x^{(t)}, y^{(t)}$), primeiro é executado o passo de *feedforward* para calcular todas as ativações na rede, incluindo o valor de saída $h_{\\Theta}(x)$. Posteriormente, para cada neurônio $j$ na camada $l$, é calculado o \"erro\" $\\delta_{j}^{(l)}$ que mede o quanto determinado neurônio contribuiu para a diferença entre o valor esperado e o valor obtido na saída da rede.\n",
    "\n",
    "<center>\n",
    "<div style=\"padding: 5px; float: center;\">\n",
    "    <img src=\"figs/nn_back.png\"  style=\"height:350px;\"/> \n",
    "    <center><em>Figura 3. Arquitetura da rede neural.</em></center>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "Nos neurônios de saída, a diferença pode ser medida entre o valor esperado (rótulo da amostra) e o valor obtido (a ativação final da rede), onde tal diferença será usada para definir $\\delta_{j}^{(3)}$ (visto que a camada 3 é a última). Nas camadas ocultas (quando houver mais de uma), o termo $\\delta_{j}^{(l)}$ será calculado com base na média ponderada dos erros encontrados na camada posterior ($l + 1$).\n",
    "\n",
    "A seguir, é descrito em detalhes como a implementação do algoritmo *backpropagation* deve ser feita. Você precisará seguir os passos 1 a 4 dentro de um laço, processando uma amostra por vez. No passo 5, o gradiente acumulado é dividido pelas *m* amostras, o qual será utilizado na função de custo da rede neural.\n",
    "\n",
    " 1. Coloque os valores na camada de entrada ($a^{(1)}$) para a amostra de treino a ser processada. Calcule as ativações das camadas 2 e 3 utilizando o passo de *feedforward*. Observe que será necessário adicionar um termo $+1$ para garantir que os vetores de ativação das camadas ($a^{(1)}$) e ($a^{(2)}$) também incluam o neurônio de *bias*.\n",
    " \n",
    " 2. Para cada neurônio $k$ na camada 3 (camada de saída), defina:\n",
    "    $$ \\delta_{k}^{(3)} = (a^{(3)}_k - y_k), $$\n",
    "    onde $y_k \\in \\{0,1\\}$ indica se a amostra sendo processada pertence a classe $k$ ($y_k = 1$) ou não ($y_k = 0$).\n",
    "    \n",
    " 3. Para a camada oculta $l$ = 2, defina:\n",
    "\n",
    "    $$ \\delta^{(2)} = (\\Theta^{(2)})^T \\delta^{(3)}*g'(z^{(2)}) $$\n",
    "    \n",
    " 4. Acumule o gradiente usando a fórmula descrita a seguir. Lembre-se de não utilizar o valor associado ao bias $\\delta^{(2)}_0$.\n",
    "    \n",
    "    $$ \\Delta^{(l)} = \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^T $$\n",
    "    \n",
    " 5. Obtenha o gradiente sem regularização para a função de custo da rede neural dividindo os gradientes acumulados por $\\frac{1}{m}$:\n",
    " \n",
    "     $$ D^{(l)}_{ij} = \\frac{1}{m}\\Delta^{(l)}_{ij} $$\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "Neste ponto, você deverá implementar o algoritmo de *backpropagation*. Crie uma nova função de custo, baseada na função implementada anteriormente, que retorne as derivadas parciais dos parâmetros. Nesta função, você precisará implementar o gradiente para a rede neural sem regularização.\n",
    "\n",
    "Logo após a função que implementa o algoritmo de *backpropagation*, é chamada uma outra função que fará a checagem do gradiente. O código dessa função está no arquivo **utils.py** que está localizado na pasta raíz desse exercício. Esta checagem tem o propósito de certificar que seu código calcula o gradiente corretamente. Neste passo, se sua implementação estiver correta, você deverá ver uma diferença **menor que 1e-9**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.76654043e-02  1.76654044e-02]\n",
      " [ 4.83605804e-04  4.83605809e-04]\n",
      " [ 7.06767167e-04  7.06767168e-04]\n",
      " [ 2.80130052e-04  2.80130052e-04]\n",
      " [ 9.70879575e-03  9.70879576e-03]\n",
      " [ 2.36708468e-04  2.36708468e-04]\n",
      " [ 2.83653863e-04  2.83653862e-04]\n",
      " [ 6.98092006e-05  6.98092035e-05]\n",
      " [-7.18128122e-03 -7.18128123e-03]\n",
      " [-2.28610917e-04 -2.28610917e-04]\n",
      " [-4.01542821e-04 -4.01542820e-04]\n",
      " [-2.05298107e-04 -2.05298106e-04]\n",
      " [-1.74827533e-02 -1.74827533e-02]\n",
      " [-4.83497640e-04 -4.83497643e-04]\n",
      " [-7.17048672e-04 -7.17048672e-04]\n",
      " [-2.91348459e-04 -2.91348459e-04]\n",
      " [-1.16920643e-02 -1.16920643e-02]\n",
      " [-2.93754241e-04 -2.93754241e-04]\n",
      " [-3.73295379e-04 -3.73295381e-04]\n",
      " [-1.09630474e-04 -1.09630470e-04]\n",
      " [ 1.09347722e-01  1.09347722e-01]\n",
      " [ 5.67965185e-02  5.67965185e-02]\n",
      " [ 5.25298306e-02  5.25298306e-02]\n",
      " [ 5.53542907e-02  5.53542907e-02]\n",
      " [ 5.59290833e-02  5.59290833e-02]\n",
      " [ 5.23534682e-02  5.23534682e-02]\n",
      " [ 1.08133003e-01  1.08133003e-01]\n",
      " [ 5.67319602e-02  5.67319602e-02]\n",
      " [ 5.14442931e-02  5.14442931e-02]\n",
      " [ 5.48296085e-02  5.48296085e-02]\n",
      " [ 5.56926532e-02  5.56926532e-02]\n",
      " [ 5.11795651e-02  5.11795651e-02]\n",
      " [ 5.06270372e-01  5.06270372e-01]\n",
      " [ 2.63880175e-01  2.63880175e-01]\n",
      " [ 2.41215476e-01  2.41215476e-01]\n",
      " [ 2.57977109e-01  2.57977109e-01]\n",
      " [ 2.58731922e-01  2.58731922e-01]\n",
      " [ 2.40983787e-01  2.40983787e-01]]\n",
      "As duas colunas acima deve ser bem semelhantes.\n",
      "(Esquerda - Gradiente numerico, Direita - Seu gradiente)\n",
      "\n",
      "Se sua implementacao de backpropagation esta correta, \n",
      "a diferenca relativa devera ser pequena (menor que 1e-9). \n",
      "\n",
      "Diferenca relativa: 1.89394e-11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def funcaoCusto_backp(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y):\n",
    "    '''\n",
    "    Implementa a funcao de custo para a rede neural com tres camadas\n",
    "    voltada para a tarefa de classificacao\n",
    "    \n",
    "    Calcula o custo e gradiente da rede neural. \n",
    "    Os parametros da rede sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    input_layer_size - tamanho da camada de entrada\n",
    "    hidden_layer_size - tamanho da camada oculta\n",
    "    num_labels - numero de classes possiveis\n",
    "    lambda - parametro de regularizacao\n",
    "    \n",
    "    O vetor grad de retorno contem todas as derivadas parciais\n",
    "    da rede neural.\n",
    "    '''\n",
    "\n",
    "    # Extrai os parametros de nn_params e alimenta as variaveis Theta1 e Theta2.\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # As variaveis a seguir precisam ser retornadas corretamente\n",
    "    J = 0;\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #               acompanhando os seguintes passos.\n",
    "    #\n",
    "    # (1): Lembre-se de transformar os rotulos Y em vetores com 10 posicoes,\n",
    "    #         onde tera zero em todas posicoes exceto na posicao do rotulo\n",
    "    #\n",
    "    # (2): Execute a etapa de feedforward e coloque o custo na variavel J.\n",
    "    #\n",
    "    # (3): Implemente o algoritmo de backpropagation para calcular \n",
    "    #      os gradientes e alimentar as variaveis Theta1_grad e Theta2_grad.\n",
    "    #\n",
    "    #\n",
    "\n",
    "    eps = 1e-15\n",
    "    #print(y)\n",
    "    y_rotulo = np.zeros((len(y),num_labels),dtype=int)\n",
    "    for i in range(len(y)) :\n",
    "        for j in range(num_labels) :\n",
    "            if j == y[i]-1 :\n",
    "                y_rotulo[i][j] = 1\n",
    "            else :\n",
    "                y_rotulo[i][j] = 0\n",
    "\n",
    "    g2 = np.zeros(Theta2[:,1:].shape)\n",
    "    \n",
    "    z2 = np.insert(X,0,1,axis = 1).dot(Theta1.T)\n",
    "\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = np.insert(a2,0,1,axis = 1).dot(Theta2.T)\n",
    "    \n",
    "    a3 = sigmoid(z3) \n",
    "    \n",
    "    J = (1/m) * sum(np.sum(((-y_rotulo) * np.log(a3 + eps)) - ((1-y_rotulo) * (np.log(1-a3 + eps) )),axis=1))\n",
    "    \n",
    "    g3 = (a3 - y_rotulo)\n",
    "    #print(a3)    \n",
    "    g2 = ((g3).dot(Theta2[:,1:]) * (sigmoidGradient(z2)))\n",
    "\n",
    "    a2i = np.insert(a2,0,1, axis = 1)\n",
    "        \n",
    "    #print(g3.shape)\n",
    "    #print(a2i.T.shape)\n",
    "    #print(Theta2_grad.shape)\n",
    "    Theta2_grad = Theta2_grad + (g3.T.dot(a2i))   \n",
    "        \n",
    "        #print(X[i].reshape(len(X[i]),1).shape)\n",
    "    xi = np.insert(X,0,1,axis = 1)\n",
    "    Theta1_grad = Theta1_grad + (g2.T.dot(xi))  \n",
    "        #print('-----------------------------')\n",
    "    Theta2_grad = Theta2_grad/m\n",
    "    Theta1_grad = Theta1_grad/m\n",
    "    #print(Theta1_grad)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    # Junta os gradientes\n",
    "    grad = np.concatenate([np.ravel(Theta1_grad), np.ravel(Theta2_grad)])\n",
    "\n",
    "    return J, grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# executa o arquivo que contem a funca que faz a checagem do gradiente\n",
    "%run utils.py\n",
    "verificaGradiente(funcaoCusto_backp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 7: Outra parte da regularização\n",
    "\n",
    "Agora, a regularização deverá ser adicionada após se calcular o gradiente durante o algoritmo de *backpropagation*. Lembre-se que a regularização não é adicionada quando $j = 0$, ou seja, na primeira coluna de $\\Theta$. Portanto, para $j \\geq 1$, o gradiente é descrito como:\n",
    "\n",
    "$$ D^{(l)}_{ij} = \\frac{1}{m}\\Delta^{(l)}_{ij} + \\frac{\\lambda}{m}\\Theta^{(l)}_{ij} $$\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "Você deverá criar uma nova função de custo que é uma atualização da função anterior, mas com regularização e gradiente.\n",
    "\n",
    "Logo após a função que implementa o algoritmo de *backpropagation* com regularização, a função que faz a checagem do gradiente será chamada novamente. Se sua implementação estiver correta, você deverá ver uma diferença **menor que 1e-9**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0176654   0.0176654 ]\n",
      " [ 0.05504145  0.05504145]\n",
      " [ 0.00917397  0.00917397]\n",
      " [-0.04512802 -0.04512802]\n",
      " [ 0.0097088   0.0097088 ]\n",
      " [-0.01652822 -0.01652822]\n",
      " [ 0.03970285  0.03970285]\n",
      " [ 0.0594313   0.0594313 ]\n",
      " [-0.00718128 -0.00718128]\n",
      " [-0.03286988 -0.03286988]\n",
      " [-0.06040096 -0.06040096]\n",
      " [-0.03239967 -0.03239967]\n",
      " [-0.01748275 -0.01748275]\n",
      " [ 0.05895294  0.05895294]\n",
      " [ 0.03830022  0.03830022]\n",
      " [-0.01756555 -0.01756555]\n",
      " [-0.01169206 -0.01169206]\n",
      " [-0.04535299 -0.04535299]\n",
      " [ 0.00861934  0.00861934]\n",
      " [ 0.05466708  0.05466708]\n",
      " [ 0.10934772  0.10934772]\n",
      " [ 0.11135436  0.11135436]\n",
      " [ 0.06099703  0.06099703]\n",
      " [ 0.00994614  0.00994614]\n",
      " [-0.00160637 -0.00160637]\n",
      " [ 0.03558854  0.03558854]\n",
      " [ 0.108133    0.108133  ]\n",
      " [ 0.11609346  0.11609346]\n",
      " [ 0.0761714   0.0761714 ]\n",
      " [ 0.02218834  0.02218834]\n",
      " [-0.00430676 -0.00430676]\n",
      " [ 0.01898519  0.01898519]\n",
      " [ 0.50627037  0.50627037]\n",
      " [ 0.32331662  0.32331662]\n",
      " [ 0.28023275  0.28023275]\n",
      " [ 0.24070291  0.24070291]\n",
      " [ 0.20104807  0.20104807]\n",
      " [ 0.19592455  0.19592455]]\n",
      "As duas colunas acima deve ser bem semelhantes.\n",
      "(Esquerda - Gradiente numerico, Direita - Seu gradiente)\n",
      "\n",
      "Se sua implementacao de backpropagation esta correta, \n",
      "a diferenca relativa devera ser pequena (menor que 1e-9). \n",
      "\n",
      "Diferenca relativa: 1.79024e-11\n",
      "\n",
      "\n",
      "\n",
      "Checando a funcao de custo (c/ regularizacao) ... \n",
      "\n",
      "Custo com os parametros (carregados do arquivo): 0.576051\n",
      "\n",
      "(este valor deve ser proximo de 0.576051 (para lambda = 3))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def funcaoCusto_backp_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, vLambda):\n",
    "    '''\n",
    "    Implementa a funcao de custo para a rede neural com tres camadas\n",
    "    voltada para tarefa de classificacao\n",
    "    \n",
    "    Calcula o custo e gradiente da rede neural. \n",
    "    Os parametros da rede neural sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    input_layer_size - tamanho da camada de entrada\n",
    "    hidden_layer_size - tamanho da camada oculta\n",
    "    num_labels - numero de classes possiveis\n",
    "    lambda - parametro de regularizacao\n",
    "    \n",
    "    O vetor grad de retorno contem todas as derivadas parciais\n",
    "    da rede neural.\n",
    "    '''\n",
    "\n",
    "    # Extrai os parametros de nn_params e alimenta as variaveis Theta1 e Theta2.\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # As variaveis a seguir precisam ser retornadas corretamente\n",
    "    J = 0;\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #               acompanhando os seguintes passos.\n",
    "    #\n",
    "    # (1): Lembre-se de transformar os rotulos Y em vetores com 10 posicoes,\n",
    "    #         onde tera zero em todas posicoes exceto na posicao do rotulo\n",
    "    #\n",
    "    # (2): Execute a etapa de feedforward e coloque o custo na variavel J.\n",
    "    #\n",
    "    # (3): Implemente o algoritmo de backpropagation para calcular \n",
    "    #      os gradientes e alimentar as variaveis Theta1_grad e Theta2_grad.\n",
    "    #\n",
    "    # (4): Implemente a regularização na função de custo e gradiente.\n",
    "    #\n",
    "\n",
    "    eps = 1e-15\n",
    "    y_rotulo = np.zeros((len(y),num_labels),dtype=int)\n",
    "    for i in range(len(y)) :\n",
    "        for j in range(num_labels) :\n",
    "            if j == y[i]-1 :\n",
    "                y_rotulo[i][j] = 1\n",
    "            else :\n",
    "                y_rotulo[i][j] = 0\n",
    "\n",
    "    z2 = np.insert(X,0,1,axis = 1).dot(Theta1.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = np.insert(a2,0,1,axis = 1).dot(Theta2.T)\n",
    "    a3 = sigmoid(z3) \n",
    "    \n",
    "    reg = sum(np.sum(np.power(Theta1[:,1:],2),axis=1)) + sum(np.sum(np.power(Theta2[:,1:],2),axis=1))\n",
    "    reg = (vLambda/(2 * m)) * (reg)      \n",
    "    \n",
    "    \n",
    "    J = (1/m) * sum(sum(((-y_rotulo) * np.log(a3 + eps)) - ((1-y_rotulo) * (np.log(1-a3 + eps) )))) + reg  \n",
    "    #print(reg)\n",
    "    #print( (1/m) * sum(sum(((-y_rotulo) * np.log(a3 + eps)) - ((1-y_rotulo) * (np.log(1-a3 + eps) )))))\n",
    "    \n",
    "    g3 = (a3 - y_rotulo)\n",
    "        \n",
    "    g2 = ((g3).dot(Theta2[:,1:]) * (sigmoidGradient(z2)))\n",
    "\n",
    "    a2i = np.insert(a2,0,1, axis = 1)\n",
    "        \n",
    "    #print(g3.shape)\n",
    "    #print(a2i.T.shape)\n",
    "    #print(Theta2_grad.shape)\n",
    "    Theta2_grad = Theta2_grad + (g3.T.dot(a2i))   \n",
    "        \n",
    "    xi = np.insert(X,0,1,axis = 1)\n",
    "    Theta1_grad = Theta1_grad + (g2.T.dot(xi))  \n",
    "\n",
    "    \n",
    "    Theta2_grad = (Theta2_grad/m) \n",
    "    Theta2_grad[:,1:] = Theta2_grad[:,1:] + ((vLambda/m)*Theta2[:,1:])\n",
    "    Theta1_grad = (Theta1_grad/m)\n",
    "    \n",
    "    Theta1_grad[:,1:] = Theta1_grad[:,1:] + ((vLambda/m)*Theta1[:,1:])\n",
    "    #print('end : ')\n",
    "    #print(np.ravel(Theta1_grad))\n",
    "    #print(J)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    # Junta os gradientes\n",
    "    grad = np.concatenate([np.ravel(Theta1_grad), np.ravel(Theta2_grad)])\n",
    "\n",
    "    return J, grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parametro de regularizacao dos pesos.\n",
    "vLambda = 3;\n",
    "\n",
    "\n",
    "# executa o arquivo que contem a funca que faz a checagem do gradiente. \n",
    "# Desa vez o valor de lambda tambem e informado\n",
    "%run utils.py\n",
    "verificaGradiente(funcaoCusto_backp_reg, vLambda=vLambda)\n",
    "\n",
    "\n",
    "print('\\n\\nChecando a funcao de custo (c/ regularizacao) ... \\n')\n",
    "\n",
    "J, grad = funcaoCusto_backp_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, X, Y, vLambda)\n",
    "\n",
    "print('Custo com os parametros (carregados do arquivo): %1.6f' %J)\n",
    "print('\\n(este valor deve ser proximo de 0.576051 (para lambda = 3))\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 8: Treinando a rede neural\n",
    "\n",
    "Neste ponto, todo o código necessário para treinar a rede está pronto.\n",
    "Aqui, será utilizada a funcao `minimize` do ScyPy para treinar as funções de custo\n",
    "de forma eficiente utilizando os gradientes calculados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treinando a rede neural.......\n",
      ".......(Aguarde, pois esse processo por ser um pouco demorado.)\n",
      "\n",
      "     fun: 0.3143628038273941\n",
      "     jac: array([ 3.22854897e-05,  2.49965943e-08,  1.55152671e-08, ...,\n",
      "       -1.05830578e-04, -8.59082266e-05, -1.08776111e-04])\n",
      " message: 'Max. number of function evaluations reached'\n",
      "    nfev: 500\n",
      "     nit: 30\n",
      "  status: 3\n",
      " success: False\n",
      "       x: array([-1.08625224e+00,  1.24982971e-04,  7.75763355e-05, ...,\n",
      "        3.58657196e-01,  2.23143957e+00,  1.04154264e+00])\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import scipy.optimize\n",
    "\n",
    "print('\\nTreinando a rede neural.......')\n",
    "print('.......(Aguarde, pois esse processo por ser um pouco demorado.)\\n')\n",
    "\n",
    "# Apos ter completado toda a tarefa, mude o parametro MaxIter para\n",
    "# um valor maior e verifique como isso afeta o treinamento.\n",
    "MaxIter = 500\n",
    "\n",
    "# Voce tambem pode testar valores diferentes para lambda.\n",
    "vLambda = 1\n",
    "\n",
    "# Minimiza a funcao de custo\n",
    "result = scipy.optimize.minimize(fun=funcaoCusto_backp_reg, x0=initial_rna_params, args=(input_layer_size, hidden_layer_size, num_labels, X, Y, vLambda),  \n",
    "                method='TNC', jac=True, options={'maxiter': MaxIter})\n",
    "\n",
    "# Coleta os pesos retornados pela função de minimização\n",
    "nn_params = result.x\n",
    "\n",
    "# Obtem Theta1 e Theta2 back a partir de rna_params\n",
    "Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 9: Visualizando os pesos\n",
    "\n",
    "\n",
    "Uma das formas de entender o que a rede neural está aprendendo, é visualizar a representação capturada nos neurônios da camada oculta. Informalmente, dado um neurônio de uma camada oculta qualquer, uma das formas de visualizar o que esse neurônio calcula, é encontrar uma entrada *x* que o fará ser ativado (ou seja, um resultado próximo a 1). Para a rede neural que foi treinada, perceba que a $i$-ésima linha de $\\Theta^{(1)}$ é um vetor com 401 dimensões, o qual representa os parâmetros para o $i$-ésimo neurônio. Se descartarmos o termo *bias*, teremos um vetor de 400 dimensões que representa o peso para cada pixel a partir da camada de entrada. Deste modo, uma das formas de visualizar a representação capturada pelo neurônio da camada oculta, é reorganizar essas 400 dimensões em uma imagem de 20 x 20 pixels e exibi-la. \n",
    "\n",
    "O script a seguir irá exibir uma imagem com 25 unidades, cada uma correspondendo a um neurônio da camada oculta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualizando a rede neural... \n",
      "\n",
      "(5000, 400)\n",
      "(25, 400)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('\\nVisualizando a rede neural... \\n')\n",
    "\n",
    "print(X.shape)\n",
    "print(Theta1[:, 1:].shape)\n",
    "\n",
    "visualizaDados(Theta1[:, 1:])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 10: Predição\n",
    "\n",
    "Após treinar a rede neural, ela será utilizada para predizer\n",
    "os rótulos das amostras. Neste ponto, foi implementada a função de predição\n",
    "para que a rede neural seja capaz de prever os rótulos no conjunto de dados\n",
    "e calcular a acurácia do método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Acuracia no conjunto de treinamento: 99.580000\n",
      "\n",
      "\n",
      "Acuracia esperada: 99.56% (aproximadamente)\n"
     ]
    }
   ],
   "source": [
    "def predicao(Theta1, Theta2, X):\n",
    "    '''\n",
    "    Prediz o rotulo de uma amostra apresentada a rede neural\n",
    "    \n",
    "    Prediz o rotulo de X ao utilizar\n",
    "    os pesos treinados na rede neural (Theta1, Theta2)\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[0] # número de amostras\n",
    "    num_labels = Theta2.shape[0]\n",
    "    \n",
    "    p = np.zeros(m)\n",
    "\n",
    "    a1 = np.hstack( [np.ones([m,1]),X] )\n",
    "    h1 = sigmoid( np.dot(a1,Theta1.T) )\n",
    "\n",
    "    a2 = np.hstack( [np.ones([m,1]),h1] ) \n",
    "    h2 = sigmoid( np.dot(a2,Theta2.T) )\n",
    "    \n",
    "    p = np.argmax(h2,axis=1)\n",
    "    p = p+1\n",
    "    \n",
    "    return p\n",
    "    \n",
    "\n",
    "pred = predicao(Theta1, Theta2, X)\n",
    "\n",
    "print('\\nAcuracia no conjunto de treinamento: %f\\n'%( np.mean( pred == Y ) * 100) )\n",
    "\n",
    "print('\\nAcuracia esperada: 99.56% (aproximadamente)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
